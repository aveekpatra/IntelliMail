{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 5, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-provider.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-chat-language-model.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/convert-to-openai-chat-messages.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/map-openai-chat-logprobs.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/map-openai-finish-reason.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-error.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/get-response-metadata.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-prepare-tools.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-completion-language-model.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/convert-to-openai-completion-prompt.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/map-openai-completion-logprobs.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-embedding-model.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-image-model.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-image-settings.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/responses/openai-responses-language-model.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/responses/convert-to-openai-responses-messages.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/responses/map-openai-responses-finish-reason.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/responses/openai-responses-prepare-tools.ts","/turbopack/[project]/node_modules/@ai-sdk/openai/src/openai-tools.ts"],"sourcesContent":["import {\n  EmbeddingModelV1,\n  ImageModelV1,\n  LanguageModelV1,\n  ProviderV1,\n} from '@ai-sdk/provider';\nimport {\n  FetchFunction,\n  loadApiKey,\n  withoutTrailingSlash,\n} from '@ai-sdk/provider-utils';\nimport { OpenAIChatLanguageModel } from './openai-chat-language-model';\nimport { OpenAIChatModelId, OpenAIChatSettings } from './openai-chat-settings';\nimport { OpenAICompletionLanguageModel } from './openai-completion-language-model';\nimport {\n  OpenAICompletionModelId,\n  OpenAICompletionSettings,\n} from './openai-completion-settings';\nimport { OpenAIEmbeddingModel } from './openai-embedding-model';\nimport {\n  OpenAIEmbeddingModelId,\n  OpenAIEmbeddingSettings,\n} from './openai-embedding-settings';\nimport { OpenAIImageModel } from './openai-image-model';\nimport {\n  OpenAIImageModelId,\n  OpenAIImageSettings,\n} from './openai-image-settings';\nimport { OpenAIResponsesLanguageModel } from './responses/openai-responses-language-model';\nimport { OpenAIResponsesModelId } from './responses/openai-responses-settings';\nimport { openaiTools } from './openai-tools';\n\nexport interface OpenAIProvider extends ProviderV1 {\n  (\n    modelId: 'gpt-3.5-turbo-instruct',\n    settings?: OpenAICompletionSettings,\n  ): OpenAICompletionLanguageModel;\n  (modelId: OpenAIChatModelId, settings?: OpenAIChatSettings): LanguageModelV1;\n\n  /**\nCreates an OpenAI model for text generation.\n   */\n  languageModel(\n    modelId: 'gpt-3.5-turbo-instruct',\n    settings?: OpenAICompletionSettings,\n  ): OpenAICompletionLanguageModel;\n  languageModel(\n    modelId: OpenAIChatModelId,\n    settings?: OpenAIChatSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates an OpenAI chat model for text generation.\n   */\n  chat(\n    modelId: OpenAIChatModelId,\n    settings?: OpenAIChatSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates an OpenAI responses API model for text generation.\n   */\n  responses(modelId: OpenAIResponsesModelId): LanguageModelV1;\n\n  /**\nCreates an OpenAI completion model for text generation.\n   */\n  completion(\n    modelId: OpenAICompletionModelId,\n    settings?: OpenAICompletionSettings,\n  ): LanguageModelV1;\n\n  /**\nCreates a model for text embeddings.\n   */\n  embedding(\n    modelId: OpenAIEmbeddingModelId,\n    settings?: OpenAIEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  /**\nCreates a model for text embeddings.\n\n@deprecated Use `textEmbeddingModel` instead.\n   */\n  textEmbedding(\n    modelId: OpenAIEmbeddingModelId,\n    settings?: OpenAIEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  /**\nCreates a model for text embeddings.\n   */\n  textEmbeddingModel(\n    modelId: OpenAIEmbeddingModelId,\n    settings?: OpenAIEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  /**\nCreates a model for image generation.\n   */\n  image(\n    modelId: OpenAIImageModelId,\n    settings?: OpenAIImageSettings,\n  ): ImageModelV1;\n\n  /**\nCreates a model for image generation.\n   */\n  imageModel(\n    modelId: OpenAIImageModelId,\n    settings?: OpenAIImageSettings,\n  ): ImageModelV1;\n\n  /**\nOpenAI-specific tools.\n   */\n  tools: typeof openaiTools;\n}\n\nexport interface OpenAIProviderSettings {\n  /**\nBase URL for the OpenAI API calls.\n     */\n  baseURL?: string;\n\n  /**\nAPI key for authenticating requests.\n     */\n  apiKey?: string;\n\n  /**\nOpenAI Organization.\n     */\n  organization?: string;\n\n  /**\nOpenAI project.\n     */\n  project?: string;\n\n  /**\nCustom headers to include in the requests.\n     */\n  headers?: Record<string, string>;\n\n  /**\nOpenAI compatibility mode. Should be set to `strict` when using the OpenAI API,\nand `compatible` when using 3rd party providers. In `compatible` mode, newer\ninformation such as streamOptions are not being sent. Defaults to 'compatible'.\n   */\n  compatibility?: 'strict' | 'compatible';\n\n  /**\nProvider name. Overrides the `openai` default name for 3rd party providers.\n   */\n  name?: string;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n    */\n  fetch?: FetchFunction;\n}\n\n/**\nCreate an OpenAI provider instance.\n */\nexport function createOpenAI(\n  options: OpenAIProviderSettings = {},\n): OpenAIProvider {\n  const baseURL =\n    withoutTrailingSlash(options.baseURL) ?? 'https://api.openai.com/v1';\n\n  // we default to compatible, because strict breaks providers like Groq:\n  const compatibility = options.compatibility ?? 'compatible';\n\n  const providerName = options.name ?? 'openai';\n\n  const getHeaders = () => ({\n    Authorization: `Bearer ${loadApiKey({\n      apiKey: options.apiKey,\n      environmentVariableName: 'OPENAI_API_KEY',\n      description: 'OpenAI',\n    })}`,\n    'OpenAI-Organization': options.organization,\n    'OpenAI-Project': options.project,\n    ...options.headers,\n  });\n\n  const createChatModel = (\n    modelId: OpenAIChatModelId,\n    settings: OpenAIChatSettings = {},\n  ) =>\n    new OpenAIChatLanguageModel(modelId, settings, {\n      provider: `${providerName}.chat`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      compatibility,\n      fetch: options.fetch,\n    });\n\n  const createCompletionModel = (\n    modelId: OpenAICompletionModelId,\n    settings: OpenAICompletionSettings = {},\n  ) =>\n    new OpenAICompletionLanguageModel(modelId, settings, {\n      provider: `${providerName}.completion`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      compatibility,\n      fetch: options.fetch,\n    });\n\n  const createEmbeddingModel = (\n    modelId: OpenAIEmbeddingModelId,\n    settings: OpenAIEmbeddingSettings = {},\n  ) =>\n    new OpenAIEmbeddingModel(modelId, settings, {\n      provider: `${providerName}.embedding`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n\n  const createImageModel = (\n    modelId: OpenAIImageModelId,\n    settings: OpenAIImageSettings = {},\n  ) =>\n    new OpenAIImageModel(modelId, settings, {\n      provider: `${providerName}.image`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n\n  const createLanguageModel = (\n    modelId: OpenAIChatModelId | OpenAICompletionModelId,\n    settings?: OpenAIChatSettings | OpenAICompletionSettings,\n  ) => {\n    if (new.target) {\n      throw new Error(\n        'The OpenAI model function cannot be called with the new keyword.',\n      );\n    }\n\n    if (modelId === 'gpt-3.5-turbo-instruct') {\n      return createCompletionModel(\n        modelId,\n        settings as OpenAICompletionSettings,\n      );\n    }\n\n    return createChatModel(modelId, settings as OpenAIChatSettings);\n  };\n\n  const createResponsesModel = (modelId: OpenAIResponsesModelId) => {\n    return new OpenAIResponsesLanguageModel(modelId, {\n      provider: `${providerName}.responses`,\n      url: ({ path }) => `${baseURL}${path}`,\n      headers: getHeaders,\n      fetch: options.fetch,\n    });\n  };\n\n  const provider = function (\n    modelId: OpenAIChatModelId | OpenAICompletionModelId,\n    settings?: OpenAIChatSettings | OpenAICompletionSettings,\n  ) {\n    return createLanguageModel(modelId, settings);\n  };\n\n  provider.languageModel = createLanguageModel;\n  provider.chat = createChatModel;\n  provider.completion = createCompletionModel;\n  provider.responses = createResponsesModel;\n  provider.embedding = createEmbeddingModel;\n  provider.textEmbedding = createEmbeddingModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n\n  provider.image = createImageModel;\n  provider.imageModel = createImageModel;\n\n  provider.tools = openaiTools;\n\n  return provider as OpenAIProvider;\n}\n\n/**\nDefault OpenAI provider instance. It uses 'strict' compatibility mode.\n */\nexport const openai = createOpenAI({\n  compatibility: 'strict', // strict for OpenAI API\n});\n","import {\n  InvalidResponseDataError,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1LogProbs,\n  LanguageModelV1ProviderMetadata,\n  LanguageModelV1StreamPart,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport {\n  FetchFunction,\n  ParseResult,\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonResponseHandler,\n  generateId,\n  isParsableJson,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAIChatMessages } from './convert-to-openai-chat-messages';\nimport { mapOpenAIChatLogProbsOutput } from './map-openai-chat-logprobs';\nimport { mapOpenAIFinishReason } from './map-openai-finish-reason';\nimport { OpenAIChatModelId, OpenAIChatSettings } from './openai-chat-settings';\nimport {\n  openaiErrorDataSchema,\n  openaiFailedResponseHandler,\n} from './openai-error';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { prepareTools } from './openai-prepare-tools';\n\ntype OpenAIChatConfig = {\n  provider: string;\n  compatibility: 'strict' | 'compatible';\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n};\n\nexport class OpenAIChatLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  readonly modelId: OpenAIChatModelId;\n  readonly settings: OpenAIChatSettings;\n\n  private readonly config: OpenAIChatConfig;\n\n  constructor(\n    modelId: OpenAIChatModelId,\n    settings: OpenAIChatSettings,\n    config: OpenAIChatConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  get supportsStructuredOutputs(): boolean {\n    // enable structured outputs for reasoning models by default:\n    // TODO in the next major version, remove this and always use json mode for models\n    // that support structured outputs (blacklist other models)\n    return this.settings.structuredOutputs ?? isReasoningModel(this.modelId);\n  }\n\n  get defaultObjectGenerationMode() {\n    // audio models don't support structured outputs:\n    if (isAudioModel(this.modelId)) {\n      return 'tool';\n    }\n\n    return this.supportsStructuredOutputs ? 'json' : 'tool';\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get supportsImageUrls(): boolean {\n    // image urls can be sent if downloadImages is disabled (default):\n    return !this.settings.downloadImages;\n  }\n\n  private getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences,\n    responseFormat,\n    seed,\n    providerMetadata,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (\n      responseFormat?.type === 'json' &&\n      responseFormat.schema != null &&\n      !this.supportsStructuredOutputs\n    ) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details:\n          'JSON response format schema is only supported with structuredOutputs',\n      });\n    }\n\n    const useLegacyFunctionCalling = this.settings.useLegacyFunctionCalling;\n\n    if (useLegacyFunctionCalling && this.settings.parallelToolCalls === true) {\n      throw new UnsupportedFunctionalityError({\n        functionality: 'useLegacyFunctionCalling with parallelToolCalls',\n      });\n    }\n\n    if (useLegacyFunctionCalling && this.supportsStructuredOutputs) {\n      throw new UnsupportedFunctionalityError({\n        functionality: 'structuredOutputs with useLegacyFunctionCalling',\n      });\n    }\n\n    const { messages, warnings: messageWarnings } = convertToOpenAIChatMessages(\n      {\n        prompt,\n        useLegacyFunctionCalling,\n        systemMessageMode: getSystemMessageMode(this.modelId),\n      },\n    );\n\n    warnings.push(...messageWarnings);\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      logit_bias: this.settings.logitBias,\n      logprobs:\n        this.settings.logprobs === true ||\n        typeof this.settings.logprobs === 'number'\n          ? true\n          : undefined,\n      top_logprobs:\n        typeof this.settings.logprobs === 'number'\n          ? this.settings.logprobs\n          : typeof this.settings.logprobs === 'boolean'\n            ? this.settings.logprobs\n              ? 0\n              : undefined\n            : undefined,\n      user: this.settings.user,\n      parallel_tool_calls: this.settings.parallelToolCalls,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format:\n        responseFormat?.type === 'json'\n          ? this.supportsStructuredOutputs && responseFormat.schema != null\n            ? {\n                type: 'json_schema',\n                json_schema: {\n                  schema: responseFormat.schema,\n                  strict: true,\n                  name: responseFormat.name ?? 'response',\n                  description: responseFormat.description,\n                },\n              }\n            : { type: 'json_object' }\n          : undefined,\n      stop: stopSequences,\n      seed,\n\n      // openai specific settings:\n      // TODO remove in next major version; we auto-map maxTokens now\n      max_completion_tokens: providerMetadata?.openai?.maxCompletionTokens,\n      store: providerMetadata?.openai?.store,\n      metadata: providerMetadata?.openai?.metadata,\n      prediction: providerMetadata?.openai?.prediction,\n      reasoning_effort:\n        providerMetadata?.openai?.reasoningEffort ??\n        this.settings.reasoningEffort,\n\n      // messages:\n      messages,\n    };\n\n    if (isReasoningModel(this.modelId)) {\n      // remove unsupported settings for reasoning models\n      // see https://platform.openai.com/docs/guides/reasoning#limitations\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'temperature',\n          details: 'temperature is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'topP',\n          details: 'topP is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.frequency_penalty != null) {\n        baseArgs.frequency_penalty = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'frequencyPenalty',\n          details: 'frequencyPenalty is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.presence_penalty != null) {\n        baseArgs.presence_penalty = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'presencePenalty',\n          details: 'presencePenalty is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.logit_bias != null) {\n        baseArgs.logit_bias = undefined;\n        warnings.push({\n          type: 'other',\n          message: 'logitBias is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.logprobs != null) {\n        baseArgs.logprobs = undefined;\n        warnings.push({\n          type: 'other',\n          message: 'logprobs is not supported for reasoning models',\n        });\n      }\n      if (baseArgs.top_logprobs != null) {\n        baseArgs.top_logprobs = undefined;\n        warnings.push({\n          type: 'other',\n          message: 'topLogprobs is not supported for reasoning models',\n        });\n      }\n\n      // reasoning models use max_completion_tokens instead of max_tokens:\n      if (baseArgs.max_tokens != null) {\n        if (baseArgs.max_completion_tokens == null) {\n          baseArgs.max_completion_tokens = baseArgs.max_tokens;\n        }\n        baseArgs.max_tokens = undefined;\n      }\n    }\n\n    switch (type) {\n      case 'regular': {\n        const { tools, tool_choice, functions, function_call, toolWarnings } =\n          prepareTools({\n            mode,\n            useLegacyFunctionCalling,\n            structuredOutputs: this.supportsStructuredOutputs,\n          });\n\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice,\n            functions,\n            function_call,\n          },\n          warnings: [...warnings, ...toolWarnings],\n        };\n      }\n\n      case 'object-json': {\n        return {\n          args: {\n            ...baseArgs,\n            response_format:\n              this.supportsStructuredOutputs && mode.schema != null\n                ? {\n                    type: 'json_schema',\n                    json_schema: {\n                      schema: mode.schema,\n                      strict: true,\n                      name: mode.name ?? 'response',\n                      description: mode.description,\n                    },\n                  }\n                : { type: 'json_object' },\n          },\n          warnings,\n        };\n      }\n\n      case 'object-tool': {\n        return {\n          args: useLegacyFunctionCalling\n            ? {\n                ...baseArgs,\n                function_call: {\n                  name: mode.tool.name,\n                },\n                functions: [\n                  {\n                    name: mode.tool.name,\n                    description: mode.tool.description,\n                    parameters: mode.tool.parameters,\n                  },\n                ],\n              }\n            : {\n                ...baseArgs,\n                tool_choice: {\n                  type: 'function',\n                  function: { name: mode.tool.name },\n                },\n                tools: [\n                  {\n                    type: 'function',\n                    function: {\n                      name: mode.tool.name,\n                      description: mode.tool.description,\n                      parameters: mode.tool.parameters,\n                      strict: this.supportsStructuredOutputs ? true : undefined,\n                    },\n                  },\n                ],\n              },\n          warnings,\n        };\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args: body, warnings } = this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiChatResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = body;\n    const choice = response.choices[0];\n\n    // provider metadata:\n    const completionTokenDetails = response.usage?.completion_tokens_details;\n    const promptTokenDetails = response.usage?.prompt_tokens_details;\n    const providerMetadata: LanguageModelV1ProviderMetadata = { openai: {} };\n    if (completionTokenDetails?.reasoning_tokens != null) {\n      providerMetadata.openai.reasoningTokens =\n        completionTokenDetails?.reasoning_tokens;\n    }\n    if (completionTokenDetails?.accepted_prediction_tokens != null) {\n      providerMetadata.openai.acceptedPredictionTokens =\n        completionTokenDetails?.accepted_prediction_tokens;\n    }\n    if (completionTokenDetails?.rejected_prediction_tokens != null) {\n      providerMetadata.openai.rejectedPredictionTokens =\n        completionTokenDetails?.rejected_prediction_tokens;\n    }\n    if (promptTokenDetails?.cached_tokens != null) {\n      providerMetadata.openai.cachedPromptTokens =\n        promptTokenDetails?.cached_tokens;\n    }\n\n    return {\n      text: choice.message.content ?? undefined,\n      toolCalls:\n        this.settings.useLegacyFunctionCalling && choice.message.function_call\n          ? [\n              {\n                toolCallType: 'function',\n                toolCallId: generateId(),\n                toolName: choice.message.function_call.name,\n                args: choice.message.function_call.arguments,\n              },\n            ]\n          : choice.message.tool_calls?.map(toolCall => ({\n              toolCallType: 'function',\n              toolCallId: toolCall.id ?? generateId(),\n              toolName: toolCall.function.name,\n              args: toolCall.function.arguments!,\n            })),\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: response.usage?.prompt_tokens ?? NaN,\n        completionTokens: response.usage?.completion_tokens ?? NaN,\n      },\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      request: { body: JSON.stringify(body) },\n      response: getResponseMetadata(response),\n      warnings,\n      logprobs: mapOpenAIChatLogProbsOutput(choice.logprobs),\n      providerMetadata,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n\n      const simulatedStream = new ReadableStream<LanguageModelV1StreamPart>({\n        start(controller) {\n          controller.enqueue({ type: 'response-metadata', ...result.response });\n          if (result.text) {\n            controller.enqueue({\n              type: 'text-delta',\n              textDelta: result.text,\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: 'tool-call-delta',\n                toolCallType: 'function',\n                toolCallId: toolCall.toolCallId,\n                toolName: toolCall.toolName,\n                argsTextDelta: toolCall.args,\n              });\n\n              controller.enqueue({\n                type: 'tool-call',\n                ...toolCall,\n              });\n            }\n          }\n          controller.enqueue({\n            type: 'finish',\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata,\n          });\n          controller.close();\n        },\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings,\n      };\n    }\n\n    const { args, warnings } = this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options:\n        this.config.compatibility === 'strict'\n          ? { include_usage: true }\n          : undefined,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiChatChunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = args;\n\n    const toolCalls: Array<{\n      id: string;\n      type: 'function';\n      function: {\n        name: string;\n        arguments: string;\n      };\n      hasFinished: boolean;\n    }> = [];\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: {\n      promptTokens: number | undefined;\n      completionTokens: number | undefined;\n    } = {\n      promptTokens: undefined,\n      completionTokens: undefined,\n    };\n    let logprobs: LanguageModelV1LogProbs;\n    let isFirstChunk = true;\n\n    const { useLegacyFunctionCalling } = this.settings;\n\n    const providerMetadata: LanguageModelV1ProviderMetadata = { openai: {} };\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof openaiChatChunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              const {\n                prompt_tokens,\n                completion_tokens,\n                prompt_tokens_details,\n                completion_tokens_details,\n              } = value.usage;\n\n              usage = {\n                promptTokens: prompt_tokens ?? undefined,\n                completionTokens: completion_tokens ?? undefined,\n              };\n\n              if (completion_tokens_details?.reasoning_tokens != null) {\n                providerMetadata.openai.reasoningTokens =\n                  completion_tokens_details?.reasoning_tokens;\n              }\n              if (\n                completion_tokens_details?.accepted_prediction_tokens != null\n              ) {\n                providerMetadata.openai.acceptedPredictionTokens =\n                  completion_tokens_details?.accepted_prediction_tokens;\n              }\n              if (\n                completion_tokens_details?.rejected_prediction_tokens != null\n              ) {\n                providerMetadata.openai.rejectedPredictionTokens =\n                  completion_tokens_details?.rejected_prediction_tokens;\n              }\n              if (prompt_tokens_details?.cached_tokens != null) {\n                providerMetadata.openai.cachedPromptTokens =\n                  prompt_tokens_details?.cached_tokens;\n              }\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n\n            if (choice?.delta == null) {\n              return;\n            }\n\n            const delta = choice.delta;\n\n            if (delta.content != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: delta.content,\n              });\n            }\n\n            const mappedLogprobs = mapOpenAIChatLogProbsOutput(\n              choice?.logprobs,\n            );\n            if (mappedLogprobs?.length) {\n              if (logprobs === undefined) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n\n            const mappedToolCalls: typeof delta.tool_calls =\n              useLegacyFunctionCalling && delta.function_call != null\n                ? [\n                    {\n                      type: 'function',\n                      id: generateId(),\n                      function: delta.function_call,\n                      index: 0,\n                    },\n                  ]\n                : delta.tool_calls;\n\n            if (mappedToolCalls != null) {\n              for (const toolCallDelta of mappedToolCalls) {\n                const index = toolCallDelta.index;\n\n                // Tool call start. OpenAI returns all information except the arguments in the first chunk.\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== 'function') {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`,\n                    });\n                  }\n\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`,\n                    });\n                  }\n\n                  if (toolCallDelta.function?.name == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`,\n                    });\n                  }\n\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: 'function',\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: toolCallDelta.function.arguments ?? '',\n                    },\n                    hasFinished: false,\n                  };\n\n                  const toolCall = toolCalls[index];\n\n                  if (\n                    toolCall.function?.name != null &&\n                    toolCall.function?.arguments != null\n                  ) {\n                    // send delta if the argument text has already started:\n                    if (toolCall.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: 'tool-call-delta',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id,\n                        toolName: toolCall.function.name,\n                        argsTextDelta: toolCall.function.arguments,\n                      });\n                    }\n\n                    // check if tool call is complete\n                    // (some providers send the full tool call in one chunk):\n                    if (isParsableJson(toolCall.function.arguments)) {\n                      controller.enqueue({\n                        type: 'tool-call',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id ?? generateId(),\n                        toolName: toolCall.function.name,\n                        args: toolCall.function.arguments,\n                      });\n                      toolCall.hasFinished = true;\n                    }\n                  }\n\n                  continue;\n                }\n\n                // existing tool call, merge if not finished\n                const toolCall = toolCalls[index];\n\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n\n                if (toolCallDelta.function?.arguments != null) {\n                  toolCall.function!.arguments +=\n                    toolCallDelta.function?.arguments ?? '';\n                }\n\n                // send delta\n                controller.enqueue({\n                  type: 'tool-call-delta',\n                  toolCallType: 'function',\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: toolCallDelta.function.arguments ?? '',\n                });\n\n                // check if tool call is complete\n                if (\n                  toolCall.function?.name != null &&\n                  toolCall.function?.arguments != null &&\n                  isParsableJson(toolCall.function.arguments)\n                ) {\n                  controller.enqueue({\n                    type: 'tool-call',\n                    toolCallType: 'function',\n                    toolCallId: toolCall.id ?? generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments,\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              logprobs,\n              usage: {\n                promptTokens: usage.promptTokens ?? NaN,\n                completionTokens: usage.completionTokens ?? NaN,\n              },\n              ...(providerMetadata != null ? { providerMetadata } : {}),\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings,\n    };\n  }\n}\n\nconst openaiTokenUsageSchema = z\n  .object({\n    prompt_tokens: z.number().nullish(),\n    completion_tokens: z.number().nullish(),\n    prompt_tokens_details: z\n      .object({\n        cached_tokens: z.number().nullish(),\n      })\n      .nullish(),\n    completion_tokens_details: z\n      .object({\n        reasoning_tokens: z.number().nullish(),\n        accepted_prediction_tokens: z.number().nullish(),\n        rejected_prediction_tokens: z.number().nullish(),\n      })\n      .nullish(),\n  })\n  .nullish();\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal('assistant').nullish(),\n        content: z.string().nullish(),\n        function_call: z\n          .object({\n            arguments: z.string(),\n            name: z.string(),\n          })\n          .nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string().nullish(),\n              type: z.literal('function'),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      index: z.number(),\n      logprobs: z\n        .object({\n          content: z\n            .array(\n              z.object({\n                token: z.string(),\n                logprob: z.number(),\n                top_logprobs: z.array(\n                  z.object({\n                    token: z.string(),\n                    logprob: z.number(),\n                  }),\n                ),\n              }),\n            )\n            .nullable(),\n        })\n        .nullish(),\n      finish_reason: z.string().nullish(),\n    }),\n  ),\n  usage: openaiTokenUsageSchema,\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiChatChunkSchema = z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        delta: z\n          .object({\n            role: z.enum(['assistant']).nullish(),\n            content: z.string().nullish(),\n            function_call: z\n              .object({\n                name: z.string().optional(),\n                arguments: z.string().optional(),\n              })\n              .nullish(),\n            tool_calls: z\n              .array(\n                z.object({\n                  index: z.number(),\n                  id: z.string().nullish(),\n                  type: z.literal('function').optional(),\n                  function: z.object({\n                    name: z.string().nullish(),\n                    arguments: z.string().nullish(),\n                  }),\n                }),\n              )\n              .nullish(),\n          })\n          .nullish(),\n        logprobs: z\n          .object({\n            content: z\n              .array(\n                z.object({\n                  token: z.string(),\n                  logprob: z.number(),\n                  top_logprobs: z.array(\n                    z.object({\n                      token: z.string(),\n                      logprob: z.number(),\n                    }),\n                  ),\n                }),\n              )\n              .nullable(),\n          })\n          .nullish(),\n        finish_reason: z.string().nullable().optional(),\n        index: z.number(),\n      }),\n    ),\n    usage: openaiTokenUsageSchema,\n  }),\n  openaiErrorDataSchema,\n]);\n\nfunction isReasoningModel(modelId: string) {\n  return (\n    modelId === 'o1' ||\n    modelId.startsWith('o1-') ||\n    modelId === 'o3' ||\n    modelId.startsWith('o3-')\n  );\n}\n\nfunction isAudioModel(modelId: string) {\n  return modelId.startsWith('gpt-4o-audio-preview');\n}\n\nfunction getSystemMessageMode(modelId: string) {\n  if (!isReasoningModel(modelId)) {\n    return 'system';\n  }\n\n  return (\n    reasoningModels[modelId as keyof typeof reasoningModels]\n      ?.systemMessageMode ?? 'developer'\n  );\n}\n\nconst reasoningModels = {\n  'o1-mini': {\n    systemMessageMode: 'remove',\n  },\n  'o1-mini-2024-09-12': {\n    systemMessageMode: 'remove',\n  },\n  'o1-preview': {\n    systemMessageMode: 'remove',\n  },\n  'o1-preview-2024-09-12': {\n    systemMessageMode: 'remove',\n  },\n  'o3-mini': {\n    systemMessageMode: 'developer',\n  },\n  'o3-mini-2025-01-31': {\n    systemMessageMode: 'developer',\n  },\n} as const;\n","import {\n  LanguageModelV1CallWarning,\n  LanguageModelV1Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { convertUint8ArrayToBase64 } from '@ai-sdk/provider-utils';\nimport { OpenAIChatPrompt } from './openai-chat-prompt';\n\nexport function convertToOpenAIChatMessages({\n  prompt,\n  useLegacyFunctionCalling = false,\n  systemMessageMode = 'system',\n}: {\n  prompt: LanguageModelV1Prompt;\n  useLegacyFunctionCalling?: boolean;\n  systemMessageMode?: 'system' | 'developer' | 'remove';\n}): {\n  messages: OpenAIChatPrompt;\n  warnings: Array<LanguageModelV1CallWarning>;\n} {\n  const messages: OpenAIChatPrompt = [];\n  const warnings: Array<LanguageModelV1CallWarning> = [];\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        switch (systemMessageMode) {\n          case 'system': {\n            messages.push({ role: 'system', content });\n            break;\n          }\n          case 'developer': {\n            messages.push({ role: 'developer', content });\n            break;\n          }\n          case 'remove': {\n            warnings.push({\n              type: 'other',\n              message: 'system messages are removed for this model',\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck: never = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`,\n            );\n          }\n        }\n        break;\n      }\n\n      case 'user': {\n        if (content.length === 1 && content[0].type === 'text') {\n          messages.push({ role: 'user', content: content[0].text });\n          break;\n        }\n\n        messages.push({\n          role: 'user',\n          content: content.map((part, index) => {\n            switch (part.type) {\n              case 'text': {\n                return { type: 'text', text: part.text };\n              }\n              case 'image': {\n                return {\n                  type: 'image_url',\n                  image_url: {\n                    url:\n                      part.image instanceof URL\n                        ? part.image.toString()\n                        : `data:${\n                            part.mimeType ?? 'image/jpeg'\n                          };base64,${convertUint8ArrayToBase64(part.image)}`,\n\n                    // OpenAI specific extension: image detail\n                    detail: part.providerMetadata?.openai?.imageDetail,\n                  },\n                };\n              }\n              case 'file': {\n                if (part.data instanceof URL) {\n                  throw new UnsupportedFunctionalityError({\n                    functionality:\n                      \"'File content parts with URL data' functionality not supported.\",\n                  });\n                }\n\n                switch (part.mimeType) {\n                  case 'audio/wav': {\n                    return {\n                      type: 'input_audio',\n                      input_audio: { data: part.data, format: 'wav' },\n                    };\n                  }\n                  case 'audio/mp3':\n                  case 'audio/mpeg': {\n                    return {\n                      type: 'input_audio',\n                      input_audio: { data: part.data, format: 'mp3' },\n                    };\n                  }\n                  case 'application/pdf': {\n                    return {\n                      type: 'file',\n                      file: {\n                        filename: part.filename ?? `part-${index}.pdf`,\n                        file_data: `data:application/pdf;base64,${part.data}`,\n                      },\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: `File content part type ${part.mimeType} in user messages`,\n                    });\n                  }\n                }\n              }\n            }\n          }),\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        let text = '';\n        const toolCalls: Array<{\n          id: string;\n          type: 'function';\n          function: { name: string; arguments: string };\n        }> = [];\n\n        for (const part of content) {\n          switch (part.type) {\n            case 'text': {\n              text += part.text;\n              break;\n            }\n            case 'tool-call': {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: 'function',\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args),\n                },\n              });\n              break;\n            }\n          }\n        }\n\n        if (useLegacyFunctionCalling) {\n          if (toolCalls.length > 1) {\n            throw new UnsupportedFunctionalityError({\n              functionality:\n                'useLegacyFunctionCalling with multiple tool calls in one message',\n            });\n          }\n\n          messages.push({\n            role: 'assistant',\n            content: text,\n            function_call:\n              toolCalls.length > 0 ? toolCalls[0].function : undefined,\n          });\n        } else {\n          messages.push({\n            role: 'assistant',\n            content: text,\n            tool_calls: toolCalls.length > 0 ? toolCalls : undefined,\n          });\n        }\n\n        break;\n      }\n\n      case 'tool': {\n        for (const toolResponse of content) {\n          if (useLegacyFunctionCalling) {\n            messages.push({\n              role: 'function',\n              name: toolResponse.toolName,\n              content: JSON.stringify(toolResponse.result),\n            });\n          } else {\n            messages.push({\n              role: 'tool',\n              tool_call_id: toolResponse.toolCallId,\n              content: JSON.stringify(toolResponse.result),\n            });\n          }\n        }\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return { messages, warnings };\n}\n","import { LanguageModelV1LogProbs } from '@ai-sdk/provider';\n\ntype OpenAIChatLogProbs = {\n  content:\n    | {\n        token: string;\n        logprob: number;\n        top_logprobs:\n          | {\n              token: string;\n              logprob: number;\n            }[]\n          | null;\n      }[]\n    | null;\n};\n\nexport function mapOpenAIChatLogProbsOutput(\n  logprobs: OpenAIChatLogProbs | null | undefined,\n): LanguageModelV1LogProbs | undefined {\n  return (\n    logprobs?.content?.map(({ token, logprob, top_logprobs }) => ({\n      token,\n      logprob,\n      topLogprobs: top_logprobs\n        ? top_logprobs.map(({ token, logprob }) => ({\n            token,\n            logprob,\n          }))\n        : [],\n    })) ?? undefined\n  );\n}\n","import { LanguageModelV1FinishReason } from '@ai-sdk/provider';\n\nexport function mapOpenAIFinishReason(\n  finishReason: string | null | undefined,\n): LanguageModelV1FinishReason {\n  switch (finishReason) {\n    case 'stop':\n      return 'stop';\n    case 'length':\n      return 'length';\n    case 'content_filter':\n      return 'content-filter';\n    case 'function_call':\n    case 'tool_calls':\n      return 'tool-calls';\n    default:\n      return 'unknown';\n  }\n}\n","import { z } from 'zod';\nimport { createJsonErrorResponseHandler } from '@ai-sdk/provider-utils';\n\nexport const openaiErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type OpenAIErrorData = z.infer<typeof openaiErrorDataSchema>;\n\nexport const openaiFailedResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: openaiErrorDataSchema,\n  errorToMessage: data => data.error.message,\n});\n","export function getResponseMetadata({\n  id,\n  model,\n  created,\n}: {\n  id?: string | undefined | null;\n  created?: number | undefined | null;\n  model?: string | undefined | null;\n}) {\n  return {\n    id: id ?? undefined,\n    modelId: model ?? undefined,\n    timestamp: created != null ? new Date(created * 1000) : undefined,\n  };\n}\n","import {\n  JSONSchema7,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function prepareTools({\n  mode,\n  useLegacyFunctionCalling = false,\n  structuredOutputs,\n}: {\n  mode: Parameters<LanguageModelV1['doGenerate']>[0]['mode'] & {\n    type: 'regular';\n  };\n  useLegacyFunctionCalling: boolean | undefined;\n  structuredOutputs: boolean;\n}): {\n  tools?: {\n    type: 'function';\n    function: {\n      name: string;\n      description: string | undefined;\n      parameters: JSONSchema7;\n      strict?: boolean;\n    };\n  }[];\n  tool_choice?:\n    | 'auto'\n    | 'none'\n    | 'required'\n    | { type: 'function'; function: { name: string } };\n\n  // legacy support\n  functions?: {\n    name: string;\n    description: string | undefined;\n    parameters: JSONSchema7;\n  }[];\n  function_call?: { name: string };\n  toolWarnings: Array<LanguageModelV1CallWarning>;\n} {\n  // when the tools array is empty, change it to undefined to prevent errors:\n  const tools = mode.tools?.length ? mode.tools : undefined;\n\n  const toolWarnings: LanguageModelV1CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, tool_choice: undefined, toolWarnings };\n  }\n\n  const toolChoice = mode.toolChoice;\n\n  if (useLegacyFunctionCalling) {\n    const openaiFunctions: Array<{\n      name: string;\n      description: string | undefined;\n      parameters: JSONSchema7;\n    }> = [];\n\n    for (const tool of tools) {\n      if (tool.type === 'provider-defined') {\n        toolWarnings.push({ type: 'unsupported-tool', tool });\n      } else {\n        openaiFunctions.push({\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n        });\n      }\n    }\n\n    if (toolChoice == null) {\n      return {\n        functions: openaiFunctions,\n        function_call: undefined,\n        toolWarnings,\n      };\n    }\n\n    const type = toolChoice.type;\n\n    switch (type) {\n      case 'auto':\n      case 'none':\n      case undefined:\n        return {\n          functions: openaiFunctions,\n          function_call: undefined,\n          toolWarnings,\n        };\n      case 'required':\n        throw new UnsupportedFunctionalityError({\n          functionality: 'useLegacyFunctionCalling and toolChoice: required',\n        });\n      default:\n        return {\n          functions: openaiFunctions,\n          function_call: { name: toolChoice.toolName },\n          toolWarnings,\n        };\n    }\n  }\n\n  const openaiTools: Array<{\n    type: 'function';\n    function: {\n      name: string;\n      description: string | undefined;\n      parameters: JSONSchema7;\n      strict: boolean | undefined;\n    };\n  }> = [];\n\n  for (const tool of tools) {\n    if (tool.type === 'provider-defined') {\n      toolWarnings.push({ type: 'unsupported-tool', tool });\n    } else {\n      openaiTools.push({\n        type: 'function',\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n          strict: structuredOutputs ? true : undefined,\n        },\n      });\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: openaiTools, tool_choice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case 'auto':\n    case 'none':\n    case 'required':\n      return { tools: openaiTools, tool_choice: type, toolWarnings };\n    case 'tool':\n      return {\n        tools: openaiTools,\n        tool_choice: {\n          type: 'function',\n          function: {\n            name: toolChoice.toolName,\n          },\n        },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n","import {\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1LogProbs,\n  LanguageModelV1StreamPart,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport {\n  FetchFunction,\n  ParseResult,\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAICompletionPrompt } from './convert-to-openai-completion-prompt';\nimport { mapOpenAICompletionLogProbs } from './map-openai-completion-logprobs';\nimport { mapOpenAIFinishReason } from './map-openai-finish-reason';\nimport {\n  OpenAICompletionModelId,\n  OpenAICompletionSettings,\n} from './openai-completion-settings';\nimport {\n  openaiErrorDataSchema,\n  openaiFailedResponseHandler,\n} from './openai-error';\nimport { getResponseMetadata } from './get-response-metadata';\n\ntype OpenAICompletionConfig = {\n  provider: string;\n  compatibility: 'strict' | 'compatible';\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n};\n\nexport class OpenAICompletionLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n  readonly defaultObjectGenerationMode = undefined;\n\n  readonly modelId: OpenAICompletionModelId;\n  readonly settings: OpenAICompletionSettings;\n\n  private readonly config: OpenAICompletionConfig;\n\n  constructor(\n    modelId: OpenAICompletionModelId,\n    settings: OpenAICompletionSettings,\n    config: OpenAICompletionConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (responseFormat != null && responseFormat.type !== 'text') {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details: 'JSON response format is not supported.',\n      });\n    }\n\n    const { prompt: completionPrompt, stopSequences } =\n      convertToOpenAICompletionPrompt({ prompt, inputFormat });\n\n    const stop = [...(stopSequences ?? []), ...(userStopSequences ?? [])];\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      logprobs:\n        typeof this.settings.logprobs === 'number'\n          ? this.settings.logprobs\n          : typeof this.settings.logprobs === 'boolean'\n            ? this.settings.logprobs\n              ? 0\n              : undefined\n            : undefined,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n\n      // prompt:\n      prompt: completionPrompt,\n\n      // stop sequences:\n      stop: stop.length > 0 ? stop : undefined,\n    };\n\n    switch (type) {\n      case 'regular': {\n        if (mode.tools?.length) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'tools',\n          });\n        }\n\n        if (mode.toolChoice) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'toolChoice',\n          });\n        }\n\n        return { args: baseArgs, warnings };\n      }\n\n      case 'object-json': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-json mode',\n        });\n      }\n\n      case 'object-tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-tool mode',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompletionResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: response.usage.prompt_tokens,\n        completionTokens: response.usage.completion_tokens,\n      },\n      finishReason: mapOpenAIFinishReason(choice.finish_reason),\n      logprobs: mapOpenAICompletionLogProbs(choice.logprobs),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(response),\n      warnings,\n      request: { body: JSON.stringify(args) },\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n\n      // only include stream_options when in strict compatibility mode:\n      stream_options:\n        this.config.compatibility === 'strict'\n          ? { include_usage: true }\n          : undefined,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiCompletionChunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: { promptTokens: number; completionTokens: number } = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN,\n    };\n    let logprobs: LanguageModelV1LogProbs;\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof openaiCompletionChunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens,\n                completionTokens: value.usage.completion_tokens,\n              };\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAIFinishReason(choice.finish_reason);\n            }\n\n            if (choice?.text != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: choice.text,\n              });\n            }\n\n            const mappedLogprobs = mapOpenAICompletionLogProbs(\n              choice?.logprobs,\n            );\n            if (mappedLogprobs?.length) {\n              if (logprobs === undefined) logprobs = [];\n              logprobs.push(...mappedLogprobs);\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              logprobs,\n              usage,\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) },\n    };\n  }\n}\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompletionResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string(),\n      logprobs: z\n        .object({\n          tokens: z.array(z.string()),\n          token_logprobs: z.array(z.number()),\n          top_logprobs: z.array(z.record(z.string(), z.number())).nullable(),\n        })\n        .nullish(),\n    }),\n  ),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number(),\n  }),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompletionChunkSchema = z.union([\n  z.object({\n    id: z.string().nullish(),\n    created: z.number().nullish(),\n    model: z.string().nullish(),\n    choices: z.array(\n      z.object({\n        text: z.string(),\n        finish_reason: z.string().nullish(),\n        index: z.number(),\n        logprobs: z\n          .object({\n            tokens: z.array(z.string()),\n            token_logprobs: z.array(z.number()),\n            top_logprobs: z.array(z.record(z.string(), z.number())).nullable(),\n          })\n          .nullish(),\n      }),\n    ),\n    usage: z\n      .object({\n        prompt_tokens: z.number(),\n        completion_tokens: z.number(),\n      })\n      .nullish(),\n  }),\n  openaiErrorDataSchema,\n]);\n","import {\n  InvalidPromptError,\n  LanguageModelV1Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function convertToOpenAICompletionPrompt({\n  prompt,\n  inputFormat,\n  user = 'user',\n  assistant = 'assistant',\n}: {\n  prompt: LanguageModelV1Prompt;\n  inputFormat: 'prompt' | 'messages';\n  user?: string;\n  assistant?: string;\n}): {\n  prompt: string;\n  stopSequences?: string[];\n} {\n  // When the user supplied a prompt input, we don't transform it:\n  if (\n    inputFormat === 'prompt' &&\n    prompt.length === 1 &&\n    prompt[0].role === 'user' &&\n    prompt[0].content.length === 1 &&\n    prompt[0].content[0].type === 'text'\n  ) {\n    return { prompt: prompt[0].content[0].text };\n  }\n\n  // otherwise transform to a chat message format:\n  let text = '';\n\n  // if first message is a system message, add it to the text:\n  if (prompt[0].role === 'system') {\n    text += `${prompt[0].content}\\n\\n`;\n    prompt = prompt.slice(1);\n  }\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        throw new InvalidPromptError({\n          message: 'Unexpected system message in prompt: ${content}',\n          prompt,\n        });\n      }\n\n      case 'user': {\n        const userMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'image': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'images',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${user}:\\n${userMessage}\\n\\n`;\n        break;\n      }\n\n      case 'assistant': {\n        const assistantMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'tool-call': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'tool-call messages',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${assistant}:\\n${assistantMessage}\\n\\n`;\n        break;\n      }\n\n      case 'tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'tool messages',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  // Assistant message prefix:\n  text += `${assistant}:\\n`;\n\n  return {\n    prompt: text,\n    stopSequences: [`\\n${user}:`],\n  };\n}\n","import { LanguageModelV1LogProbs } from '@ai-sdk/provider';\n\ntype OpenAICompletionLogProps = {\n  tokens: string[];\n  token_logprobs: number[];\n  top_logprobs: Record<string, number>[] | null;\n};\n\nexport function mapOpenAICompletionLogProbs(\n  logprobs: OpenAICompletionLogProps | null | undefined,\n): LanguageModelV1LogProbs | undefined {\n  return logprobs?.tokens.map((token, index) => ({\n    token,\n    logprob: logprobs.token_logprobs[index],\n    topLogprobs: logprobs.top_logprobs\n      ? Object.entries(logprobs.top_logprobs[index]).map(\n          ([token, logprob]) => ({\n            token,\n            logprob,\n          }),\n        )\n      : [],\n  }));\n}\n","import {\n  EmbeddingModelV1,\n  TooManyEmbeddingValuesForCallError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { OpenAIConfig } from './openai-config';\nimport {\n  OpenAIEmbeddingModelId,\n  OpenAIEmbeddingSettings,\n} from './openai-embedding-settings';\nimport { openaiFailedResponseHandler } from './openai-error';\n\nexport class OpenAIEmbeddingModel implements EmbeddingModelV1<string> {\n  readonly specificationVersion = 'v1';\n  readonly modelId: OpenAIEmbeddingModelId;\n\n  private readonly config: OpenAIConfig;\n  private readonly settings: OpenAIEmbeddingSettings;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get maxEmbeddingsPerCall(): number {\n    return this.settings.maxEmbeddingsPerCall ?? 2048;\n  }\n\n  get supportsParallelCalls(): boolean {\n    return this.settings.supportsParallelCalls ?? true;\n  }\n\n  constructor(\n    modelId: OpenAIEmbeddingModelId,\n    settings: OpenAIEmbeddingSettings,\n    config: OpenAIConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n  }: Parameters<EmbeddingModelV1<string>['doEmbed']>[0]): Promise<\n    Awaited<ReturnType<EmbeddingModelV1<string>['doEmbed']>>\n  > {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values,\n      });\n    }\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/embeddings',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: 'float',\n        dimensions: this.settings.dimensions,\n        user: this.settings.user,\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTextEmbeddingResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage\n        ? { tokens: response.usage.prompt_tokens }\n        : undefined,\n      rawResponse: { headers: responseHeaders },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiTextEmbeddingResponseSchema = z.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish(),\n});\n","import { ImageModelV1, ImageModelV1CallWarning } from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { OpenAIConfig } from './openai-config';\nimport { openaiFailedResponseHandler } from './openai-error';\nimport {\n  OpenAIImageModelId,\n  OpenAIImageSettings,\n  modelMaxImagesPerCall,\n} from './openai-image-settings';\n\ninterface OpenAIImageModelConfig extends OpenAIConfig {\n  _internal?: {\n    currentDate?: () => Date;\n  };\n}\n\nexport class OpenAIImageModel implements ImageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  get maxImagesPerCall(): number {\n    return (\n      this.settings.maxImagesPerCall ?? modelMaxImagesPerCall[this.modelId] ?? 1\n    );\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  constructor(\n    readonly modelId: OpenAIImageModelId,\n    private readonly settings: OpenAIImageSettings,\n    private readonly config: OpenAIImageModelConfig,\n  ) {}\n\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal,\n  }: Parameters<ImageModelV1['doGenerate']>[0]): Promise<\n    Awaited<ReturnType<ImageModelV1['doGenerate']>>\n  > {\n    const warnings: Array<ImageModelV1CallWarning> = [];\n\n    if (aspectRatio != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'aspectRatio',\n        details:\n          'This model does not support aspect ratio. Use `size` instead.',\n      });\n    }\n\n    if (seed != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'seed' });\n    }\n\n    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: '/images/generations',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(providerOptions.openai ?? {}),\n        response_format: 'b64_json',\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiImageResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n      },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() })),\n});\n","export type OpenAIImageModelId = 'dall-e-3' | 'dall-e-2' | (string & {});\n\n// https://platform.openai.com/docs/guides/images\nexport const modelMaxImagesPerCall: Record<OpenAIImageModelId, number> = {\n  'dall-e-3': 1,\n  'dall-e-2': 10,\n};\n\nexport interface OpenAIImageSettings {\n  /**\nOverride the maximum number of images per call (default is dependent on the\nmodel, or 1 for an unknown model).\n   */\n  maxImagesPerCall?: number;\n}\n","import {\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonResponseHandler,\n  generateId,\n  parseProviderOptions,\n  ParseResult,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { OpenAIConfig } from '../openai-config';\nimport { openaiFailedResponseHandler } from '../openai-error';\nimport { convertToOpenAIResponsesMessages } from './convert-to-openai-responses-messages';\nimport { mapOpenAIResponseFinishReason } from './map-openai-responses-finish-reason';\nimport { prepareResponsesTools } from './openai-responses-prepare-tools';\nimport { OpenAIResponsesModelId } from './openai-responses-settings';\n\nexport class OpenAIResponsesLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n  readonly defaultObjectGenerationMode = 'json';\n\n  readonly modelId: OpenAIResponsesModelId;\n\n  private readonly config: OpenAIConfig;\n\n  constructor(modelId: OpenAIResponsesModelId, config: OpenAIConfig) {\n    this.modelId = modelId;\n    this.config = config;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private getArgs({\n    mode,\n    maxTokens,\n    temperature,\n    stopSequences,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    prompt,\n    providerMetadata,\n    responseFormat,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const warnings: LanguageModelV1CallWarning[] = [];\n    const modelConfig = getResponsesModelConfig(this.modelId);\n    const type = mode.type;\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (seed != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'seed',\n      });\n    }\n\n    if (presencePenalty != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'presencePenalty',\n      });\n    }\n\n    if (frequencyPenalty != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'frequencyPenalty',\n      });\n    }\n\n    if (stopSequences != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'stopSequences',\n      });\n    }\n\n    const { messages, warnings: messageWarnings } =\n      convertToOpenAIResponsesMessages({\n        prompt,\n        systemMessageMode: modelConfig.systemMessageMode,\n      });\n\n    warnings.push(...messageWarnings);\n\n    const openaiOptions = parseProviderOptions({\n      provider: 'openai',\n      providerOptions: providerMetadata,\n      schema: openaiResponsesProviderOptionsSchema,\n    });\n\n    const isStrict = openaiOptions?.strictSchemas ?? true;\n\n    const baseArgs = {\n      model: this.modelId,\n      input: messages,\n      temperature,\n      top_p: topP,\n      max_output_tokens: maxTokens,\n\n      ...(responseFormat?.type === 'json' && {\n        text: {\n          format:\n            responseFormat.schema != null\n              ? {\n                  type: 'json_schema',\n                  strict: isStrict,\n                  name: responseFormat.name ?? 'response',\n                  description: responseFormat.description,\n                  schema: responseFormat.schema,\n                }\n              : { type: 'json_object' },\n        },\n      }),\n\n      // provider options:\n      metadata: openaiOptions?.metadata,\n      parallel_tool_calls: openaiOptions?.parallelToolCalls,\n      previous_response_id: openaiOptions?.previousResponseId,\n      store: openaiOptions?.store,\n      user: openaiOptions?.user,\n      instructions: openaiOptions?.instructions,\n\n      // model-specific settings:\n      ...(modelConfig.isReasoningModel &&\n        openaiOptions?.reasoningEffort != null && {\n          reasoning: { effort: openaiOptions?.reasoningEffort },\n        }),\n      ...(modelConfig.requiredAutoTruncation && {\n        truncation: 'auto',\n      }),\n    };\n\n    if (modelConfig.isReasoningModel) {\n      // remove unsupported settings for reasoning models\n      // see https://platform.openai.com/docs/guides/reasoning#limitations\n      if (baseArgs.temperature != null) {\n        baseArgs.temperature = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'temperature',\n          details: 'temperature is not supported for reasoning models',\n        });\n      }\n\n      if (baseArgs.top_p != null) {\n        baseArgs.top_p = undefined;\n        warnings.push({\n          type: 'unsupported-setting',\n          setting: 'topP',\n          details: 'topP is not supported for reasoning models',\n        });\n      }\n    }\n\n    switch (type) {\n      case 'regular': {\n        const { tools, tool_choice, toolWarnings } = prepareResponsesTools({\n          mode,\n          strict: isStrict, // TODO support provider options on tools\n        });\n\n        return {\n          args: {\n            ...baseArgs,\n            tools,\n            tool_choice,\n          },\n          warnings: [...warnings, ...toolWarnings],\n        };\n      }\n\n      case 'object-json': {\n        return {\n          args: {\n            ...baseArgs,\n            text: {\n              format:\n                mode.schema != null\n                  ? {\n                      type: 'json_schema',\n                      strict: isStrict,\n                      name: mode.name ?? 'response',\n                      description: mode.description,\n                      schema: mode.schema,\n                    }\n                  : { type: 'json_object' },\n            },\n          },\n          warnings,\n        };\n      }\n\n      case 'object-tool': {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: { type: 'function', name: mode.tool.name },\n            tools: [\n              {\n                type: 'function',\n                name: mode.tool.name,\n                description: mode.tool.description,\n                parameters: mode.tool.parameters,\n                strict: isStrict,\n              },\n            ],\n          },\n          warnings,\n        };\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args: body, warnings } = this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/responses',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        z.object({\n          id: z.string(),\n          created_at: z.number(),\n          model: z.string(),\n          output: z.array(\n            z.discriminatedUnion('type', [\n              z.object({\n                type: z.literal('message'),\n                role: z.literal('assistant'),\n                content: z.array(\n                  z.object({\n                    type: z.literal('output_text'),\n                    text: z.string(),\n                    annotations: z.array(\n                      z.object({\n                        type: z.literal('url_citation'),\n                        start_index: z.number(),\n                        end_index: z.number(),\n                        url: z.string(),\n                        title: z.string(),\n                      }),\n                    ),\n                  }),\n                ),\n              }),\n              z.object({\n                type: z.literal('function_call'),\n                call_id: z.string(),\n                name: z.string(),\n                arguments: z.string(),\n              }),\n              z.object({\n                type: z.literal('web_search_call'),\n              }),\n              z.object({\n                type: z.literal('computer_call'),\n              }),\n              z.object({\n                type: z.literal('reasoning'),\n              }),\n            ]),\n          ),\n          incomplete_details: z.object({ reason: z.string() }).nullable(),\n          usage: usageSchema,\n        }),\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const outputTextElements = response.output\n      .filter(output => output.type === 'message')\n      .flatMap(output => output.content)\n      .filter(content => content.type === 'output_text');\n\n    const toolCalls = response.output\n      .filter(output => output.type === 'function_call')\n      .map(output => ({\n        toolCallType: 'function' as const,\n        toolCallId: output.call_id,\n        toolName: output.name,\n        args: output.arguments,\n      }));\n\n    return {\n      text: outputTextElements.map(content => content.text).join('\\n'),\n      sources: outputTextElements.flatMap(content =>\n        content.annotations.map(annotation => ({\n          sourceType: 'url',\n          id: this.config.generateId?.() ?? generateId(),\n          url: annotation.url,\n          title: annotation.title,\n        })),\n      ),\n      finishReason: mapOpenAIResponseFinishReason({\n        finishReason: response.incomplete_details?.reason,\n        hasToolCalls: toolCalls.length > 0,\n      }),\n      toolCalls: toolCalls.length > 0 ? toolCalls : undefined,\n      usage: {\n        promptTokens: response.usage.input_tokens,\n        completionTokens: response.usage.output_tokens,\n      },\n      rawCall: {\n        rawPrompt: undefined,\n        rawSettings: {},\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse,\n      },\n      request: {\n        body: JSON.stringify(body),\n      },\n      response: {\n        id: response.id,\n        timestamp: new Date(response.created_at * 1000),\n        modelId: response.model,\n      },\n      providerMetadata: {\n        openai: {\n          responseId: response.id,\n          cachedPromptTokens:\n            response.usage.input_tokens_details?.cached_tokens ?? null,\n          reasoningTokens:\n            response.usage.output_tokens_details?.reasoning_tokens ?? null,\n        },\n      },\n      warnings,\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    const { args: body, warnings } = this.getArgs(options);\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/responses',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: {\n        ...body,\n        stream: true,\n      },\n      failedResponseHandler: openaiFailedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        openaiResponsesChunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const self = this;\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let promptTokens = NaN;\n    let completionTokens = NaN;\n    let cachedPromptTokens: number | null = null;\n    let reasoningTokens: number | null = null;\n    let responseId: string | null = null;\n    const ongoingToolCalls: Record<\n      number,\n      { toolName: string; toolCallId: string } | undefined\n    > = {};\n    let hasToolCalls = false;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof openaiResponsesChunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            if (isResponseOutputItemAddedChunk(value)) {\n              if (value.item.type === 'function_call') {\n                ongoingToolCalls[value.output_index] = {\n                  toolName: value.item.name,\n                  toolCallId: value.item.call_id,\n                };\n\n                controller.enqueue({\n                  type: 'tool-call-delta',\n                  toolCallType: 'function',\n                  toolCallId: value.item.call_id,\n                  toolName: value.item.name,\n                  argsTextDelta: value.item.arguments,\n                });\n              }\n            } else if (isResponseFunctionCallArgumentsDeltaChunk(value)) {\n              const toolCall = ongoingToolCalls[value.output_index];\n\n              if (toolCall != null) {\n                controller.enqueue({\n                  type: 'tool-call-delta',\n                  toolCallType: 'function',\n                  toolCallId: toolCall.toolCallId,\n                  toolName: toolCall.toolName,\n                  argsTextDelta: value.delta,\n                });\n              }\n            } else if (isResponseCreatedChunk(value)) {\n              responseId = value.response.id;\n              controller.enqueue({\n                type: 'response-metadata',\n                id: value.response.id,\n                timestamp: new Date(value.response.created_at * 1000),\n                modelId: value.response.model,\n              });\n            } else if (isTextDeltaChunk(value)) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: value.delta,\n              });\n            } else if (\n              isResponseOutputItemDoneChunk(value) &&\n              value.item.type === 'function_call'\n            ) {\n              ongoingToolCalls[value.output_index] = undefined;\n              hasToolCalls = true;\n              controller.enqueue({\n                type: 'tool-call',\n                toolCallType: 'function',\n                toolCallId: value.item.call_id,\n                toolName: value.item.name,\n                args: value.item.arguments,\n              });\n            } else if (isResponseFinishedChunk(value)) {\n              finishReason = mapOpenAIResponseFinishReason({\n                finishReason: value.response.incomplete_details?.reason,\n                hasToolCalls,\n              });\n              promptTokens = value.response.usage.input_tokens;\n              completionTokens = value.response.usage.output_tokens;\n              cachedPromptTokens =\n                value.response.usage.input_tokens_details?.cached_tokens ??\n                cachedPromptTokens;\n              reasoningTokens =\n                value.response.usage.output_tokens_details?.reasoning_tokens ??\n                reasoningTokens;\n            } else if (isResponseAnnotationAddedChunk(value)) {\n              controller.enqueue({\n                type: 'source',\n                source: {\n                  sourceType: 'url',\n                  id: self.config.generateId?.() ?? generateId(),\n                  url: value.annotation.url,\n                  title: value.annotation.title,\n                },\n              });\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage: { promptTokens, completionTokens },\n              ...((cachedPromptTokens != null || reasoningTokens != null) && {\n                providerMetadata: {\n                  openai: {\n                    responseId,\n                    cachedPromptTokens,\n                    reasoningTokens,\n                  },\n                },\n              }),\n            });\n          },\n        }),\n      ),\n      rawCall: {\n        rawPrompt: undefined,\n        rawSettings: {},\n      },\n      rawResponse: { headers: responseHeaders },\n      request: { body: JSON.stringify(body) },\n      warnings,\n    };\n  }\n}\n\nconst usageSchema = z.object({\n  input_tokens: z.number(),\n  input_tokens_details: z\n    .object({ cached_tokens: z.number().nullish() })\n    .nullish(),\n  output_tokens: z.number(),\n  output_tokens_details: z\n    .object({ reasoning_tokens: z.number().nullish() })\n    .nullish(),\n});\n\nconst textDeltaChunkSchema = z.object({\n  type: z.literal('response.output_text.delta'),\n  delta: z.string(),\n});\n\nconst responseFinishedChunkSchema = z.object({\n  type: z.enum(['response.completed', 'response.incomplete']),\n  response: z.object({\n    incomplete_details: z.object({ reason: z.string() }).nullish(),\n    usage: usageSchema,\n  }),\n});\n\nconst responseCreatedChunkSchema = z.object({\n  type: z.literal('response.created'),\n  response: z.object({\n    id: z.string(),\n    created_at: z.number(),\n    model: z.string(),\n  }),\n});\n\nconst responseOutputItemDoneSchema = z.object({\n  type: z.literal('response.output_item.done'),\n  output_index: z.number(),\n  item: z.discriminatedUnion('type', [\n    z.object({\n      type: z.literal('message'),\n    }),\n    z.object({\n      type: z.literal('function_call'),\n      id: z.string(),\n      call_id: z.string(),\n      name: z.string(),\n      arguments: z.string(),\n      status: z.literal('completed'),\n    }),\n  ]),\n});\n\nconst responseFunctionCallArgumentsDeltaSchema = z.object({\n  type: z.literal('response.function_call_arguments.delta'),\n  item_id: z.string(),\n  output_index: z.number(),\n  delta: z.string(),\n});\n\nconst responseOutputItemAddedSchema = z.object({\n  type: z.literal('response.output_item.added'),\n  output_index: z.number(),\n  item: z.discriminatedUnion('type', [\n    z.object({\n      type: z.literal('message'),\n    }),\n    z.object({\n      type: z.literal('function_call'),\n      id: z.string(),\n      call_id: z.string(),\n      name: z.string(),\n      arguments: z.string(),\n    }),\n  ]),\n});\n\nconst responseAnnotationAddedSchema = z.object({\n  type: z.literal('response.output_text.annotation.added'),\n  annotation: z.object({\n    type: z.literal('url_citation'),\n    url: z.string(),\n    title: z.string(),\n  }),\n});\n\nconst openaiResponsesChunkSchema = z.union([\n  textDeltaChunkSchema,\n  responseFinishedChunkSchema,\n  responseCreatedChunkSchema,\n  responseOutputItemDoneSchema,\n  responseFunctionCallArgumentsDeltaSchema,\n  responseOutputItemAddedSchema,\n  responseAnnotationAddedSchema,\n  z.object({ type: z.string() }).passthrough(), // fallback for unknown chunks\n]);\n\nfunction isTextDeltaChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof textDeltaChunkSchema> {\n  return chunk.type === 'response.output_text.delta';\n}\n\nfunction isResponseOutputItemDoneChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseOutputItemDoneSchema> {\n  return chunk.type === 'response.output_item.done';\n}\n\nfunction isResponseFinishedChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseFinishedChunkSchema> {\n  return (\n    chunk.type === 'response.completed' || chunk.type === 'response.incomplete'\n  );\n}\n\nfunction isResponseCreatedChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseCreatedChunkSchema> {\n  return chunk.type === 'response.created';\n}\n\nfunction isResponseFunctionCallArgumentsDeltaChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseFunctionCallArgumentsDeltaSchema> {\n  return chunk.type === 'response.function_call_arguments.delta';\n}\n\nfunction isResponseOutputItemAddedChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseOutputItemAddedSchema> {\n  return chunk.type === 'response.output_item.added';\n}\n\nfunction isResponseAnnotationAddedChunk(\n  chunk: z.infer<typeof openaiResponsesChunkSchema>,\n): chunk is z.infer<typeof responseAnnotationAddedSchema> {\n  return chunk.type === 'response.output_text.annotation.added';\n}\n\ntype ResponsesModelConfig = {\n  isReasoningModel: boolean;\n  systemMessageMode: 'remove' | 'system' | 'developer';\n  requiredAutoTruncation: boolean;\n};\n\nfunction getResponsesModelConfig(modelId: string): ResponsesModelConfig {\n  // o series reasoning models:\n  if (modelId.startsWith('o')) {\n    if (modelId.startsWith('o1-mini') || modelId.startsWith('o1-preview')) {\n      return {\n        isReasoningModel: true,\n        systemMessageMode: 'remove',\n        requiredAutoTruncation: false,\n      };\n    }\n\n    return {\n      isReasoningModel: true,\n      systemMessageMode: 'developer',\n      requiredAutoTruncation: false,\n    };\n  }\n\n  // gpt models:\n  return {\n    isReasoningModel: false,\n    systemMessageMode: 'system',\n    requiredAutoTruncation: false,\n  };\n}\n\nconst openaiResponsesProviderOptionsSchema = z.object({\n  metadata: z.any().nullish(),\n  parallelToolCalls: z.boolean().nullish(),\n  previousResponseId: z.string().nullish(),\n  store: z.boolean().nullish(),\n  user: z.string().nullish(),\n  reasoningEffort: z.string().nullish(),\n  strictSchemas: z.boolean().nullish(),\n  instructions: z.string().nullish(),\n});\n\nexport type OpenAIResponsesProviderOptions = z.infer<\n  typeof openaiResponsesProviderOptionsSchema\n>;\n","import {\n  LanguageModelV1CallWarning,\n  LanguageModelV1Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { convertUint8ArrayToBase64 } from '@ai-sdk/provider-utils';\nimport { OpenAIResponsesPrompt } from './openai-responses-api-types';\n\nexport function convertToOpenAIResponsesMessages({\n  prompt,\n  systemMessageMode,\n}: {\n  prompt: LanguageModelV1Prompt;\n  systemMessageMode: 'system' | 'developer' | 'remove';\n}): {\n  messages: OpenAIResponsesPrompt;\n  warnings: Array<LanguageModelV1CallWarning>;\n} {\n  const messages: OpenAIResponsesPrompt = [];\n  const warnings: Array<LanguageModelV1CallWarning> = [];\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        switch (systemMessageMode) {\n          case 'system': {\n            messages.push({ role: 'system', content });\n            break;\n          }\n          case 'developer': {\n            messages.push({ role: 'developer', content });\n            break;\n          }\n          case 'remove': {\n            warnings.push({\n              type: 'other',\n              message: 'system messages are removed for this model',\n            });\n            break;\n          }\n          default: {\n            const _exhaustiveCheck: never = systemMessageMode;\n            throw new Error(\n              `Unsupported system message mode: ${_exhaustiveCheck}`,\n            );\n          }\n        }\n        break;\n      }\n\n      case 'user': {\n        messages.push({\n          role: 'user',\n          content: content.map((part, index) => {\n            switch (part.type) {\n              case 'text': {\n                return { type: 'input_text', text: part.text };\n              }\n              case 'image': {\n                return {\n                  type: 'input_image',\n                  image_url:\n                    part.image instanceof URL\n                      ? part.image.toString()\n                      : `data:${\n                          part.mimeType ?? 'image/jpeg'\n                        };base64,${convertUint8ArrayToBase64(part.image)}`,\n\n                  // OpenAI specific extension: image detail\n                  detail: part.providerMetadata?.openai?.imageDetail,\n                };\n              }\n              case 'file': {\n                if (part.data instanceof URL) {\n                  // The AI SDK automatically downloads files for user file parts with URLs\n                  throw new UnsupportedFunctionalityError({\n                    functionality: 'File URLs in user messages',\n                  });\n                }\n\n                switch (part.mimeType) {\n                  case 'application/pdf': {\n                    return {\n                      type: 'input_file',\n                      filename: part.filename ?? `part-${index}.pdf`,\n                      file_data: `data:application/pdf;base64,${part.data}`,\n                    };\n                  }\n                  default: {\n                    throw new UnsupportedFunctionalityError({\n                      functionality:\n                        'Only PDF files are supported in user messages',\n                    });\n                  }\n                }\n              }\n            }\n          }),\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        for (const part of content) {\n          switch (part.type) {\n            case 'text': {\n              messages.push({\n                role: 'assistant',\n                content: [{ type: 'output_text', text: part.text }],\n              });\n              break;\n            }\n            case 'tool-call': {\n              messages.push({\n                type: 'function_call',\n                call_id: part.toolCallId,\n                name: part.toolName,\n                arguments: JSON.stringify(part.args),\n              });\n              break;\n            }\n          }\n        }\n\n        break;\n      }\n\n      case 'tool': {\n        for (const part of content) {\n          messages.push({\n            type: 'function_call_output',\n            call_id: part.toolCallId,\n            output: JSON.stringify(part.result),\n          });\n        }\n\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return { messages, warnings };\n}\n","import { LanguageModelV1FinishReason } from '@ai-sdk/provider';\n\nexport function mapOpenAIResponseFinishReason({\n  finishReason,\n  hasToolCalls,\n}: {\n  finishReason: string | null | undefined;\n  hasToolCalls: boolean;\n}): LanguageModelV1FinishReason {\n  switch (finishReason) {\n    case undefined:\n    case null:\n      return hasToolCalls ? 'tool-calls' : 'stop';\n    case 'max_output_tokens':\n      return 'length';\n    case 'content_filter':\n      return 'content-filter';\n    default:\n      return hasToolCalls ? 'tool-calls' : 'unknown';\n  }\n}\n","import {\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { OpenAIResponsesTool } from './openai-responses-api-types';\n\nexport function prepareResponsesTools({\n  mode,\n  strict,\n}: {\n  mode: Parameters<LanguageModelV1['doGenerate']>[0]['mode'] & {\n    type: 'regular';\n  };\n  strict: boolean;\n}): {\n  tools?: Array<OpenAIResponsesTool>;\n  tool_choice?:\n    | 'auto'\n    | 'none'\n    | 'required'\n    | { type: 'function'; name: string };\n  toolWarnings: LanguageModelV1CallWarning[];\n} {\n  // when the tools array is empty, change it to undefined to prevent errors:\n  const tools = mode.tools?.length ? mode.tools : undefined;\n\n  const toolWarnings: LanguageModelV1CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, tool_choice: undefined, toolWarnings };\n  }\n\n  const toolChoice = mode.toolChoice;\n\n  const openaiTools: Array<OpenAIResponsesTool> = [];\n\n  for (const tool of tools) {\n    switch (tool.type) {\n      case 'function':\n        openaiTools.push({\n          type: 'function',\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n          strict: strict ? true : undefined,\n        });\n        break;\n      case 'provider-defined':\n        switch (tool.id) {\n          case 'openai.web_search_preview':\n            openaiTools.push({\n              type: 'web_search_preview',\n              search_context_size: tool.args.searchContextSize as\n                | 'low'\n                | 'medium'\n                | 'high',\n              user_location: tool.args.userLocation as {\n                type: 'approximate';\n                city: string;\n                region: string;\n              },\n            });\n            break;\n          default:\n            toolWarnings.push({ type: 'unsupported-tool', tool });\n            break;\n        }\n        break;\n      default:\n        toolWarnings.push({ type: 'unsupported-tool', tool });\n        break;\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: openaiTools, tool_choice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case 'auto':\n    case 'none':\n    case 'required':\n      return { tools: openaiTools, tool_choice: type, toolWarnings };\n    case 'tool':\n      return {\n        tools: openaiTools,\n        tool_choice: {\n          type: 'function',\n          name: toolChoice.toolName,\n        },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n","import { z } from 'zod';\n\nconst WebSearchPreviewParameters = z.object({});\n\nfunction webSearchPreviewTool({\n  searchContextSize,\n  userLocation,\n}: {\n  searchContextSize?: 'low' | 'medium' | 'high';\n  userLocation?: {\n    type?: 'approximate';\n    city?: string;\n    region?: string;\n    country?: string;\n    timezone?: string;\n  };\n} = {}): {\n  type: 'provider-defined';\n  id: 'openai.web_search_preview';\n  args: {};\n  parameters: typeof WebSearchPreviewParameters;\n} {\n  return {\n    type: 'provider-defined',\n    id: 'openai.web_search_preview',\n    args: {\n      searchContextSize,\n      userLocation,\n    },\n    parameters: WebSearchPreviewParameters,\n  };\n}\n\nexport const openaiTools = {\n  webSearchPreview: webSearchPreviewTool,\n};\n"],"names":["token","logprob","type","UnsupportedFunctionalityError","openaiTools","_a","toolCall","z","postJsonToApi","combineHeaders","createJsonResponseHandler","createEventSourceResponseHandler","convertUint8ArrayToBase64","_b","_c","generateId"],"mappings":";;;;;;;;;;;;;;;AEQO,SAAS,4BAA4B,EAC1C,MAAA,EACA,2BAA2B,KAAA,EAC3B,oBAAoB,QAAA,EACtB;IAQE,MAAM,WAA6B,EAAC;IACpC,MAAM,WAA8C,EAAC;IAErD,KAAA,MAAW,EAAE,IAAA,EAAM,OAAA,EAAQ,IAAK,OAAQ;QACtC,OAAQ;YACN,KAAK;gBAAU;oBACb,OAAQ;wBACN,KAAK;4BAAU;gCACb,SAAS,IAAA,CAAK;oCAAE,MAAM;oCAAU;gCAAQ;gCACxC;4BACF;wBACA,KAAK;4BAAa;gCAChB,SAAS,IAAA,CAAK;oCAAE,MAAM;oCAAa;gCAAQ;gCAC3C;4BACF;wBACA,KAAK;4BAAU;gCACb,SAAS,IAAA,CAAK;oCACZ,MAAM;oCACN,SAAS;gCACX;gCACA;4BACF;wBACA;4BAAS;gCACP,MAAM,mBAA0B;gCAChC,MAAM,IAAI,MACR,CAAA,iCAAA,EAAoC,iBAAgB,CAAA;4BAExD;oBACF;oBACA;gBACF;YAEA,KAAK;gBAAQ;oBACX,IAAI,QAAQ,MAAA,KAAW,KAAK,OAAA,CAAQ,EAAC,CAAE,IAAA,KAAS,QAAQ;wBACtD,SAAS,IAAA,CAAK;4BAAE,MAAM;4BAAQ,SAAS,OAAA,CAAQ,EAAC,CAAE,IAAA;wBAAK;wBACvD;oBACF;oBAEA,SAAS,IAAA,CAAK;wBACZ,MAAM;wBACN,SAAS,QAAQ,GAAA,CAAI,CAAC,MAAM;4BA5DtC,IAAA,IAAA,IAAA,IAAA;4BA6DY,OAAQ,KAAK,IAAA;gCACX,KAAK;oCAAQ;wCACX,OAAO;4CAAE,MAAM;4CAAQ,MAAM,KAAK,IAAA;wCAAK;oCACzC;gCACA,KAAK;oCAAS;wCACZ,OAAO;4CACL,MAAM;4CACN,WAAW;gDACT,KACE,KAAK,KAAA,YAAiB,MAClB,KAAK,KAAA,CAAM,QAAA,KACX,CAAA,KAAA,EAAA,CACE,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,aACnB,QAAA,EAAW,CAAA,GAAA,kKAAA,CAAA,4BAAA,EAA0B,KAAK,KAAK,EAAC,CAAA;gDAAA,0CAAA;gDAGtD,QAAA,CAAQ,KAAA,CAAA,KAAA,KAAK,gBAAA,KAAL,OAAA,KAAA,IAAA,GAAuB,MAAA,KAAvB,OAAA,KAAA,IAAA,GAA+B,WAAA;4CACzC;wCACF;oCACF;gCACA,KAAK;oCAAQ;wCACX,IAAI,KAAK,IAAA,YAAgB,KAAK;4CAC5B,MAAM,IAAI,yJAAA,CAAA,gCAAA,CAA8B;gDACtC,eACE;4CACJ;wCACF;wCAEA,OAAQ,KAAK,QAAA;4CACX,KAAK;gDAAa;oDAChB,OAAO;wDACL,MAAM;wDACN,aAAa;4DAAE,MAAM,KAAK,IAAA;4DAAM,QAAQ;wDAAM;oDAChD;gDACF;4CACA,KAAK;4CACL,KAAK;gDAAc;oDACjB,OAAO;wDACL,MAAM;wDACN,aAAa;4DAAE,MAAM,KAAK,IAAA;4DAAM,QAAQ;wDAAM;oDAChD;gDACF;4CACA,KAAK;gDAAmB;oDACtB,OAAO;wDACL,MAAM;wDACN,MAAM;4DACJ,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,CAAA,KAAA,EAAQ,MAAK,IAAA,CAAA;4DACxC,WAAW,CAAA,4BAAA,EAA+B,KAAK,IAAI,CAAA,CAAA;wDACrD;oDACF;gDACF;4CACA;gDAAS;oDACP,MAAM,IAAI,yJAAA,CAAA,gCAAA,CAA8B;wDACtC,eAAe,CAAA,uBAAA,EAA0B,KAAK,QAAQ,CAAA,iBAAA,CAAA;oDACxD;gDACF;wCACF;oCACF;4BACF;wBACF;oBACF;oBAEA;gBACF;YAEA,KAAK;gBAAa;oBAChB,IAAI,OAAO;oBACX,MAAM,YAID,EAAC;oBAEN,KAAA,MAAW,QAAQ,QAAS;wBAC1B,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,QAAQ,KAAK,IAAA;oCACb;gCACF;4BACA,KAAK;gCAAa;oCAChB,UAAU,IAAA,CAAK;wCACb,IAAI,KAAK,UAAA;wCACT,MAAM;wCACN,UAAU;4CACR,MAAM,KAAK,QAAA;4CACX,WAAW,KAAK,SAAA,CAAU,KAAK,IAAI;wCACrC;oCACF;oCACA;gCACF;wBACF;oBACF;oBAEA,IAAI,0BAA0B;wBAC5B,IAAI,UAAU,MAAA,GAAS,GAAG;4BACxB,MAAM,IAAI,yJAAA,CAAA,gCAAA,CAA8B;gCACtC,eACE;4BACJ;wBACF;wBAEA,SAAS,IAAA,CAAK;4BACZ,MAAM;4BACN,SAAS;4BACT,eACE,UAAU,MAAA,GAAS,IAAI,SAAA,CAAU,EAAC,CAAE,QAAA,GAAW,KAAA;wBACnD;oBACF,OAAO;wBACL,SAAS,IAAA,CAAK;4BACZ,MAAM;4BACN,SAAS;4BACT,YAAY,UAAU,MAAA,GAAS,IAAI,YAAY,KAAA;wBACjD;oBACF;oBAEA;gBACF;YAEA,KAAK;gBAAQ;oBACX,KAAA,MAAW,gBAAgB,QAAS;wBAClC,IAAI,0BAA0B;4BAC5B,SAAS,IAAA,CAAK;gCACZ,MAAM;gCACN,MAAM,aAAa,QAAA;gCACnB,SAAS,KAAK,SAAA,CAAU,aAAa,MAAM;4BAC7C;wBACF,OAAO;4BACL,SAAS,IAAA,CAAK;gCACZ,MAAM;gCACN,cAAc,aAAa,UAAA;gCAC3B,SAAS,KAAK,SAAA,CAAU,aAAa,MAAM;4BAC7C;wBACF;oBACF;oBACA;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAEA,OAAO;QAAE;QAAU;IAAS;AAC9B;;AC7LO,SAAS,4BACd,QAAA;IAlBF,IAAA,IAAA;IAoBE,OAAA,CACE,KAAA,CAAA,KAAA,YAAA,OAAA,KAAA,IAAA,SAAU,OAAA,KAAV,OAAA,KAAA,IAAA,GAAmB,GAAA,CAAI,CAAC,EAAE,KAAA,EAAO,OAAA,EAAS,YAAA,EAAa,GAAA,CAAO;YAC5D;YACA;YACA,aAAa,eACT,aAAa,GAAA,CAAI,CAAC,EAAE,OAAAA,MAAAA,EAAO,SAAAC,QAAAA,EAAQ,GAAA,CAAO;oBACxC,OAAAD;oBACA,SAAAC;gBACF,CAAA,KACA,EAAC;QACP,CAAA,EAAA,KATA,OAAA,KASO,KAAA;AAEX;;AC9BO,SAAS,sBACd,YAAA;IAEA,OAAQ;QACN,KAAK;YACH,OAAO;QACT,KAAK;YACH,OAAO;QACT,KAAK;YACH,OAAO;QACT,KAAK;QACL,KAAK;YACH,OAAO;QACT;YACE,OAAO;IACX;AACF;;;ACfO,IAAM,wBAAwB,oIAAA,CAAA,IAAA,CAAE,MAAA,CAAO;IAC5C,OAAO,oIAAA,CAAA,IAAA,CAAE,MAAA,CAAO;QACd,SAAS,oIAAA,CAAA,IAAA,CAAE,MAAA;QAAO,iEAAA;QAAA,iEAAA;QAAA,aAAA;QAKlB,MAAM,oIAAA,CAAA,IAAA,CAAE,MAAA,GAAS,OAAA;QACjB,OAAO,oIAAA,CAAA,IAAA,CAAE,GAAA,GAAM,OAAA;QACf,MAAM,oIAAA,CAAA,IAAA,CAAE,KAAA,CAAM;YAAC,oIAAA,CAAA,IAAA,CAAE,MAAA;YAAU,oIAAA,CAAA,IAAA,CAAE,MAAA;SAAS,EAAE,OAAA;IAC1C;AACF;AAIO,IAAM,8BAA8B,CAAA,GAAA,kKAAA,CAAA,iCAAA,EAA+B;IACxE,aAAa;IACb,gBAAgB,CAAA,OAAQ,KAAK,KAAA,CAAM,OAAA;AACrC;;ACrBO,SAAS,oBAAoB,EAClC,EAAA,EACA,KAAA,EACA,OAAA,EACF;IAKE,OAAO;QACL,IAAI,MAAA,OAAA,KAAM,KAAA;QACV,SAAS,SAAA,OAAA,QAAS,KAAA;QAClB,WAAW,WAAW,OAAO,IAAI,KAAK,UAAU,OAAQ,KAAA;IAC1D;AACF;;ACPO,SAAS,aAAa,EAC3B,IAAA,EACA,2BAA2B,KAAA,EAC3B,iBAAA,EACF;IAXA,IAAA;IA2CE,MAAM,QAAA,CAAA,CAAQ,KAAA,KAAK,KAAA,KAAL,OAAA,KAAA,IAAA,GAAY,MAAA,IAAS,KAAK,KAAA,GAAQ,KAAA;IAEhD,MAAM,eAA6C,EAAC;IAEpD,IAAI,SAAS,MAAM;QACjB,OAAO;YAAE,OAAO,KAAA;YAAW,aAAa,KAAA;YAAW;QAAa;IAClE;IAEA,MAAM,aAAa,KAAK,UAAA;IAExB,IAAI,0BAA0B;QAC5B,MAAM,kBAID,EAAC;QAEN,KAAA,MAAW,QAAQ,MAAO;YACxB,IAAI,KAAK,IAAA,KAAS,oBAAoB;gBACpC,aAAa,IAAA,CAAK;oBAAE,MAAM;oBAAoB;gBAAK;YACrD,OAAO;gBACL,gBAAgB,IAAA,CAAK;oBACnB,MAAM,KAAK,IAAA;oBACX,aAAa,KAAK,WAAA;oBAClB,YAAY,KAAK,UAAA;gBACnB;YACF;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,OAAO;gBACL,WAAW;gBACX,eAAe,KAAA;gBACf;YACF;QACF;QAEA,MAAMC,QAAO,WAAW,IAAA;QAExB,OAAQA;YACN,KAAK;YACL,KAAK;YACL,KAAK,KAAA;gBACH,OAAO;oBACL,WAAW;oBACX,eAAe,KAAA;oBACf;gBACF;YACF,KAAK;gBACH,MAAM,IAAIC,yJAAAA,CAAAA,gCAAAA,CAA8B;oBACtC,eAAe;gBACjB;YACF;gBACE,OAAO;oBACL,WAAW;oBACX,eAAe;wBAAE,MAAM,WAAW,QAAA;oBAAS;oBAC3C;gBACF;QACJ;IACF;IAEA,MAAMC,eAQD,EAAC;IAEN,KAAA,MAAW,QAAQ,MAAO;QACxB,IAAI,KAAK,IAAA,KAAS,oBAAoB;YACpC,aAAa,IAAA,CAAK;gBAAE,MAAM;gBAAoB;YAAK;QACrD,OAAO;YACLA,aAAY,IAAA,CAAK;gBACf,MAAM;gBACN,UAAU;oBACR,MAAM,KAAK,IAAA;oBACX,aAAa,KAAK,WAAA;oBAClB,YAAY,KAAK,UAAA;oBACjB,QAAQ,oBAAoB,OAAO,KAAA;gBACrC;YACF;QACF;IACF;IAEA,IAAI,cAAc,MAAM;QACtB,OAAO;YAAE,OAAOA;YAAa,aAAa,KAAA;YAAW;QAAa;IACpE;IAEA,MAAM,OAAO,WAAW,IAAA;IAExB,OAAQ;QACN,KAAK;QACL,KAAK;QACL,KAAK;YACH,OAAO;gBAAE,OAAOA;gBAAa,aAAa;gBAAM;YAAa;QAC/D,KAAK;YACH,OAAO;gBACL,OAAOA;gBACP,aAAa;oBACX,MAAM;oBACN,UAAU;wBACR,MAAM,WAAW,QAAA;oBACnB;gBACF;gBACA;YACF;QACF;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAID,yJAAAA,CAAAA,gCAAAA,CAA8B;oBACtC,eAAe,CAAA,8BAAA,EAAiC,iBAAgB,CAAA;gBAClE;YACF;IACF;AACF;;ANvHO,IAAM,0BAAN;IAQL,YACE,OAAA,EACA,QAAA,EACA,MAAA,CACA;QAXF,IAAA,CAAS,oBAAA,GAAuB;QAY9B,IAAA,CAAK,OAAA,GAAU;QACf,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,MAAA,GAAS;IAChB;IAEA,IAAI,4BAAqC;QA1D3C,IAAA;QA8DI,OAAA,CAAO,KAAA,IAAA,CAAK,QAAA,CAAS,iBAAA,KAAd,OAAA,KAAmC,iBAAiB,IAAA,CAAK,OAAO;IACzE;IAEA,IAAI,8BAA8B;QAEhC,IAAI,aAAa,IAAA,CAAK,OAAO,GAAG;YAC9B,OAAO;QACT;QAEA,OAAO,IAAA,CAAK,yBAAA,GAA4B,SAAS;IACnD;IAEA,IAAI,WAAmB;QACrB,OAAO,IAAA,CAAK,MAAA,CAAO,QAAA;IACrB;IAEA,IAAI,oBAA6B;QAE/B,OAAO,CAAC,IAAA,CAAK,QAAA,CAAS,cAAA;IACxB;IAEQ,QAAQ,EACd,IAAA,EACA,MAAA,EACA,SAAA,EACA,WAAA,EACA,IAAA,EACA,IAAA,EACA,gBAAA,EACA,eAAA,EACA,aAAA,EACA,cAAA,EACA,IAAA,EACA,gBAAA,EACF,EAAiD;QAhGnD,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;QAiGI,MAAM,OAAO,KAAK,IAAA;QAElB,MAAM,WAAyC,EAAC;QAEhD,IAAI,QAAQ,MAAM;YAChB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAA,CACE,kBAAA,OAAA,KAAA,IAAA,eAAgB,IAAA,MAAS,UACzB,eAAe,MAAA,IAAU,QACzB,CAAC,IAAA,CAAK,yBAAA,EACN;YACA,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;gBACT,SACE;YACJ;QACF;QAEA,MAAM,2BAA2B,IAAA,CAAK,QAAA,CAAS,wBAAA;QAE/C,IAAI,4BAA4B,IAAA,CAAK,QAAA,CAAS,iBAAA,KAAsB,MAAM;YACxE,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;gBACtC,eAAe;YACjB;QACF;QAEA,IAAI,4BAA4B,IAAA,CAAK,yBAAA,EAA2B;YAC9D,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;gBACtC,eAAe;YACjB;QACF;QAEA,MAAM,EAAE,QAAA,EAAU,UAAU,eAAA,EAAgB,GAAI,4BAC9C;YACE;YACA;YACA,mBAAmB,qBAAqB,IAAA,CAAK,OAAO;QACtD;QAGF,SAAS,IAAA,IAAQ;QAEjB,MAAM,WAAW;YAAA,YAAA;YAEf,OAAO,IAAA,CAAK,OAAA;YAAA,2BAAA;YAGZ,YAAY,IAAA,CAAK,QAAA,CAAS,SAAA;YAC1B,UACE,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,QAC3B,OAAO,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,WAC9B,OACA,KAAA;YACN,cACE,OAAO,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,WAC9B,IAAA,CAAK,QAAA,CAAS,QAAA,GACd,OAAO,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,YAChC,IAAA,CAAK,QAAA,CAAS,QAAA,GACZ,IACA,KAAA,IACF,KAAA;YACR,MAAM,IAAA,CAAK,QAAA,CAAS,IAAA;YACpB,qBAAqB,IAAA,CAAK,QAAA,CAAS,iBAAA;YAAA,yBAAA;YAGnC,YAAY;YACZ;YACA,OAAO;YACP,mBAAmB;YACnB,kBAAkB;YAClB,iBAAA,CACE,kBAAA,OAAA,KAAA,IAAA,eAAgB,IAAA,MAAS,SACrB,IAAA,CAAK,yBAAA,IAA6B,eAAe,MAAA,IAAU,OACzD;gBACE,MAAM;gBACN,aAAa;oBACX,QAAQ,eAAe,MAAA;oBACvB,QAAQ;oBACR,MAAA,CAAM,KAAA,eAAe,IAAA,KAAf,OAAA,KAAuB;oBAC7B,aAAa,eAAe,WAAA;gBAC9B;YACF,IACA;gBAAE,MAAM;YAAc,IACxB,KAAA;YACN,MAAM;YACN;YAAA,4BAAA;YAAA,+DAAA;YAIA,uBAAA,CAAuB,KAAA,oBAAA,OAAA,KAAA,IAAA,iBAAkB,MAAA,KAAlB,OAAA,KAAA,IAAA,GAA0B,mBAAA;YACjD,OAAA,CAAO,KAAA,oBAAA,OAAA,KAAA,IAAA,iBAAkB,MAAA,KAAlB,OAAA,KAAA,IAAA,GAA0B,KAAA;YACjC,UAAA,CAAU,KAAA,oBAAA,OAAA,KAAA,IAAA,iBAAkB,MAAA,KAAlB,OAAA,KAAA,IAAA,GAA0B,QAAA;YACpC,YAAA,CAAY,KAAA,oBAAA,OAAA,KAAA,IAAA,iBAAkB,MAAA,KAAlB,OAAA,KAAA,IAAA,GAA0B,UAAA;YACtC,kBAAA,CACE,KAAA,CAAA,KAAA,oBAAA,OAAA,KAAA,IAAA,iBAAkB,MAAA,KAAlB,OAAA,KAAA,IAAA,GAA0B,eAAA,KAA1B,OAAA,KACA,IAAA,CAAK,QAAA,CAAS,eAAA;YAAA,YAAA;YAGhB;QACF;QAEA,IAAI,iBAAiB,IAAA,CAAK,OAAO,GAAG;YAGlC,IAAI,SAAS,WAAA,IAAe,MAAM;gBAChC,SAAS,WAAA,GAAc,KAAA;gBACvB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;YACA,IAAI,SAAS,KAAA,IAAS,MAAM;gBAC1B,SAAS,KAAA,GAAQ,KAAA;gBACjB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;YACA,IAAI,SAAS,iBAAA,IAAqB,MAAM;gBACtC,SAAS,iBAAA,GAAoB,KAAA;gBAC7B,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;YACA,IAAI,SAAS,gBAAA,IAAoB,MAAM;gBACrC,SAAS,gBAAA,GAAmB,KAAA;gBAC5B,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;YACA,IAAI,SAAS,UAAA,IAAc,MAAM;gBAC/B,SAAS,UAAA,GAAa,KAAA;gBACtB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;gBACX;YACF;YACA,IAAI,SAAS,QAAA,IAAY,MAAM;gBAC7B,SAAS,QAAA,GAAW,KAAA;gBACpB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;gBACX;YACF;YACA,IAAI,SAAS,YAAA,IAAgB,MAAM;gBACjC,SAAS,YAAA,GAAe,KAAA;gBACxB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;gBACX;YACF;YAGA,IAAI,SAAS,UAAA,IAAc,MAAM;gBAC/B,IAAI,SAAS,qBAAA,IAAyB,MAAM;oBAC1C,SAAS,qBAAA,GAAwB,SAAS,UAAA;gBAC5C;gBACA,SAAS,UAAA,GAAa,KAAA;YACxB;QACF;QAEA,OAAQ;YACN,KAAK;gBAAW;oBACd,MAAM,EAAE,KAAA,EAAO,WAAA,EAAa,SAAA,EAAW,aAAA,EAAe,YAAA,EAAa,GACjE,aAAa;wBACX;wBACA;wBACA,mBAAmB,IAAA,CAAK,yBAAA;oBAC1B;oBAEF,OAAO;wBACL,MAAM;4BACJ,GAAG,QAAA;4BACH;4BACA;4BACA;4BACA;wBACF;wBACA,UAAU;+BAAI;+BAAa;yBAAY;oBACzC;gBACF;YAEA,KAAK;gBAAe;oBAClB,OAAO;wBACL,MAAM;4BACJ,GAAG,QAAA;4BACH,iBACE,IAAA,CAAK,yBAAA,IAA6B,KAAK,MAAA,IAAU,OAC7C;gCACE,MAAM;gCACN,aAAa;oCACX,QAAQ,KAAK,MAAA;oCACb,QAAQ;oCACR,MAAA,CAAM,KAAA,KAAK,IAAA,KAAL,OAAA,KAAa;oCACnB,aAAa,KAAK,WAAA;gCACpB;4BACF,IACA;gCAAE,MAAM;4BAAc;wBAC9B;wBACA;oBACF;gBACF;YAEA,KAAK;gBAAe;oBAClB,OAAO;wBACL,MAAM,2BACF;4BACE,GAAG,QAAA;4BACH,eAAe;gCACb,MAAM,KAAK,IAAA,CAAK,IAAA;4BAClB;4BACA,WAAW;gCACT;oCACE,MAAM,KAAK,IAAA,CAAK,IAAA;oCAChB,aAAa,KAAK,IAAA,CAAK,WAAA;oCACvB,YAAY,KAAK,IAAA,CAAK,UAAA;gCACxB;6BACF;wBACF,IACA;4BACE,GAAG,QAAA;4BACH,aAAa;gCACX,MAAM;gCACN,UAAU;oCAAE,MAAM,KAAK,IAAA,CAAK,IAAA;gCAAK;4BACnC;4BACA,OAAO;gCACL;oCACE,MAAM;oCACN,UAAU;wCACR,MAAM,KAAK,IAAA,CAAK,IAAA;wCAChB,aAAa,KAAK,IAAA,CAAK,WAAA;wCACvB,YAAY,KAAK,IAAA,CAAK,UAAA;wCACtB,QAAQ,IAAA,CAAK,yBAAA,GAA4B,OAAO,KAAA;oCAClD;gCACF;6BACF;wBACF;wBACJ;oBACF;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAEA,MAAM,WACJ,OAAA,EAC6D;QAvWjE,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;QAwWI,MAAM,EAAE,MAAM,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAE9C,MAAM,EACJ,eAAA,EACA,OAAO,QAAA,EACP,UAAU,WAAA,EACZ,GAAI,MAAM,CAAA,GAAA,kKAAA,CAAA,gBAAA,EAAc;YACtB,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAAS,CAAA,GAAA,kKAAA,CAAA,iBAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D;YACA,uBAAuB;YACvB,2BAA2B,CAAA,GAAA,kKAAA,CAAA,4BAAA,EACzB;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,EAAE,UAAU,SAAA,EAAW,GAAG,aAAY,GAAI;QAChD,MAAM,SAAS,SAAS,OAAA,CAAQ,EAAC;QAGjC,MAAM,yBAAA,CAAyB,KAAA,SAAS,KAAA,KAAT,OAAA,KAAA,IAAA,GAAgB,yBAAA;QAC/C,MAAM,qBAAA,CAAqB,KAAA,SAAS,KAAA,KAAT,OAAA,KAAA,IAAA,GAAgB,qBAAA;QAC3C,MAAM,mBAAoD;YAAE,QAAQ,CAAC;QAAE;QACvE,IAAA,CAAI,0BAAA,OAAA,KAAA,IAAA,uBAAwB,gBAAA,KAAoB,MAAM;YACpD,iBAAiB,MAAA,CAAO,eAAA,GACtB,0BAAA,OAAA,KAAA,IAAA,uBAAwB,gBAAA;QAC5B;QACA,IAAA,CAAI,0BAAA,OAAA,KAAA,IAAA,uBAAwB,0BAAA,KAA8B,MAAM;YAC9D,iBAAiB,MAAA,CAAO,wBAAA,GACtB,0BAAA,OAAA,KAAA,IAAA,uBAAwB,0BAAA;QAC5B;QACA,IAAA,CAAI,0BAAA,OAAA,KAAA,IAAA,uBAAwB,0BAAA,KAA8B,MAAM;YAC9D,iBAAiB,MAAA,CAAO,wBAAA,GACtB,0BAAA,OAAA,KAAA,IAAA,uBAAwB,0BAAA;QAC5B;QACA,IAAA,CAAI,sBAAA,OAAA,KAAA,IAAA,mBAAoB,aAAA,KAAiB,MAAM;YAC7C,iBAAiB,MAAA,CAAO,kBAAA,GACtB,sBAAA,OAAA,KAAA,IAAA,mBAAoB,aAAA;QACxB;QAEA,OAAO;YACL,MAAA,CAAM,KAAA,OAAO,OAAA,CAAQ,OAAA,KAAf,OAAA,KAA0B,KAAA;YAChC,WACE,IAAA,CAAK,QAAA,CAAS,wBAAA,IAA4B,OAAO,OAAA,CAAQ,aAAA,GACrD;gBACE;oBACE,cAAc;oBACd,YAAY,CAAA,GAAA,kKAAA,CAAA,aAAA;oBACZ,UAAU,OAAO,OAAA,CAAQ,aAAA,CAAc,IAAA;oBACvC,MAAM,OAAO,OAAA,CAAQ,aAAA,CAAc,SAAA;gBACrC;aACF,GAAA,CACA,KAAA,OAAO,OAAA,CAAQ,UAAA,KAAf,OAAA,KAAA,IAAA,GAA2B,GAAA,CAAI,CAAA;gBAja3C,IAAAE;gBAiawD,OAAA;oBAC1C,cAAc;oBACd,YAAA,CAAYA,MAAA,SAAS,EAAA,KAAT,OAAAA,MAAe,CAAA,GAAA,kKAAA,CAAA,aAAA;oBAC3B,UAAU,SAAS,QAAA,CAAS,IAAA;oBAC5B,MAAM,SAAS,QAAA,CAAS,SAAA;gBAC1B;YAAA;YACN,cAAc,sBAAsB,OAAO,aAAa;YACxD,OAAO;gBACL,cAAA,CAAc,KAAA,CAAA,KAAA,SAAS,KAAA,KAAT,OAAA,KAAA,IAAA,GAAgB,aAAA,KAAhB,OAAA,KAAiC;gBAC/C,kBAAA,CAAkB,KAAA,CAAA,KAAA,SAAS,KAAA,KAAT,OAAA,KAAA,IAAA,GAAgB,iBAAA,KAAhB,OAAA,KAAqC;YACzD;YACA,SAAS;gBAAE;gBAAW;YAAY;YAClC,aAAa;gBAAE,SAAS;gBAAiB,MAAM;YAAY;YAC3D,SAAS;gBAAE,MAAM,KAAK,SAAA,CAAU;YAAM;YACtC,UAAU,oBAAoB;YAC9B;YACA,UAAU,4BAA4B,OAAO,QAAQ;YACrD;QACF;IACF;IAEA,MAAM,SACJ,OAAA,EAC2D;QAC3D,IAAI,IAAA,CAAK,QAAA,CAAS,iBAAA,EAAmB;YACnC,MAAM,SAAS,MAAM,IAAA,CAAK,UAAA,CAAW;YAErC,MAAM,kBAAkB,IAAI,eAA0C;gBACpE,OAAM,UAAA;oBACJ,WAAW,OAAA,CAAQ;wBAAE,MAAM;wBAAqB,GAAG,OAAO,QAAA;oBAAS;oBACnE,IAAI,OAAO,IAAA,EAAM;wBACf,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,WAAW,OAAO,IAAA;wBACpB;oBACF;oBACA,IAAI,OAAO,SAAA,EAAW;wBACpB,KAAA,MAAW,YAAY,OAAO,SAAA,CAAW;4BACvC,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,cAAc;gCACd,YAAY,SAAS,UAAA;gCACrB,UAAU,SAAS,QAAA;gCACnB,eAAe,SAAS,IAAA;4BAC1B;4BAEA,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,GAAG,QAAA;4BACL;wBACF;oBACF;oBACA,WAAW,OAAA,CAAQ;wBACjB,MAAM;wBACN,cAAc,OAAO,YAAA;wBACrB,OAAO,OAAO,KAAA;wBACd,UAAU,OAAO,QAAA;wBACjB,kBAAkB,OAAO,gBAAA;oBAC3B;oBACA,WAAW,KAAA;gBACb;YACF;YACA,OAAO;gBACL,QAAQ;gBACR,SAAS,OAAO,OAAA;gBAChB,aAAa,OAAO,WAAA;gBACpB,UAAU,OAAO,QAAA;YACnB;QACF;QAEA,MAAM,EAAE,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAExC,MAAM,OAAO;YACX,GAAG,IAAA;YACH,QAAQ;YAAA,iEAAA;YAGR,gBACE,IAAA,CAAK,MAAA,CAAO,aAAA,KAAkB,WAC1B;gBAAE,eAAe;YAAK,IACtB,KAAA;QACR;QAEA,MAAM,EAAE,eAAA,EAAiB,OAAO,QAAA,EAAS,GAAI,MAAM,CAAA,GAAA,kKAAA,CAAA,gBAAA,EAAc;YAC/D,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAAS,CAAA,GAAA,kKAAA,CAAA,iBAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D;YACA,uBAAuB;YACvB,2BAA2B,CAAA,GAAA,kKAAA,CAAA,mCAAA,EACzB;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,EAAE,UAAU,SAAA,EAAW,GAAG,aAAY,GAAI;QAEhD,MAAM,YAQD,EAAC;QAEN,IAAI,eAA4C;QAChD,IAAI,QAGA;YACF,cAAc,KAAA;YACd,kBAAkB,KAAA;QACpB;QACA,IAAI;QACJ,IAAI,eAAe;QAEnB,MAAM,EAAE,wBAAA,EAAyB,GAAI,IAAA,CAAK,QAAA;QAE1C,MAAM,mBAAoD;YAAE,QAAQ,CAAC;QAAE;QAEvE,OAAO;YACL,QAAQ,SAAS,WAAA,CACf,IAAI,gBAGF;gBACA,WAAU,KAAA,EAAO,UAAA;oBApiB3B,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oBAsiBY,IAAI,CAAC,MAAM,OAAA,EAAS;wBAClB,eAAe;wBACf,WAAW,OAAA,CAAQ;4BAAE,MAAM;4BAAS,OAAO,MAAM,KAAA;wBAAM;wBACvD;oBACF;oBAEA,MAAM,QAAQ,MAAM,KAAA;oBAGpB,IAAI,WAAW,OAAO;wBACpB,eAAe;wBACf,WAAW,OAAA,CAAQ;4BAAE,MAAM;4BAAS,OAAO,MAAM,KAAA;wBAAM;wBACvD;oBACF;oBAEA,IAAI,cAAc;wBAChB,eAAe;wBAEf,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,GAAG,oBAAoB,MAAK;wBAC9B;oBACF;oBAEA,IAAI,MAAM,KAAA,IAAS,MAAM;wBACvB,MAAM,EACJ,aAAA,EACA,iBAAA,EACA,qBAAA,EACA,yBAAA,EACF,GAAI,MAAM,KAAA;wBAEV,QAAQ;4BACN,cAAc,iBAAA,OAAA,gBAAiB,KAAA;4BAC/B,kBAAkB,qBAAA,OAAA,oBAAqB,KAAA;wBACzC;wBAEA,IAAA,CAAI,6BAAA,OAAA,KAAA,IAAA,0BAA2B,gBAAA,KAAoB,MAAM;4BACvD,iBAAiB,MAAA,CAAO,eAAA,GACtB,6BAAA,OAAA,KAAA,IAAA,0BAA2B,gBAAA;wBAC/B;wBACA,IAAA,CACE,6BAAA,OAAA,KAAA,IAAA,0BAA2B,0BAAA,KAA8B,MACzD;4BACA,iBAAiB,MAAA,CAAO,wBAAA,GACtB,6BAAA,OAAA,KAAA,IAAA,0BAA2B,0BAAA;wBAC/B;wBACA,IAAA,CACE,6BAAA,OAAA,KAAA,IAAA,0BAA2B,0BAAA,KAA8B,MACzD;4BACA,iBAAiB,MAAA,CAAO,wBAAA,GACtB,6BAAA,OAAA,KAAA,IAAA,0BAA2B,0BAAA;wBAC/B;wBACA,IAAA,CAAI,yBAAA,OAAA,KAAA,IAAA,sBAAuB,aAAA,KAAiB,MAAM;4BAChD,iBAAiB,MAAA,CAAO,kBAAA,GACtB,yBAAA,OAAA,KAAA,IAAA,sBAAuB,aAAA;wBAC3B;oBACF;oBAEA,MAAM,SAAS,MAAM,OAAA,CAAQ,EAAC;oBAE9B,IAAA,CAAI,UAAA,OAAA,KAAA,IAAA,OAAQ,aAAA,KAAiB,MAAM;wBACjC,eAAe,sBAAsB,OAAO,aAAa;oBAC3D;oBAEA,IAAA,CAAI,UAAA,OAAA,KAAA,IAAA,OAAQ,KAAA,KAAS,MAAM;wBACzB;oBACF;oBAEA,MAAM,QAAQ,OAAO,KAAA;oBAErB,IAAI,MAAM,OAAA,IAAW,MAAM;wBACzB,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,WAAW,MAAM,OAAA;wBACnB;oBACF;oBAEA,MAAM,iBAAiB,4BACrB,UAAA,OAAA,KAAA,IAAA,OAAQ,QAAA;oBAEV,IAAI,kBAAA,OAAA,KAAA,IAAA,eAAgB,MAAA,EAAQ;wBAC1B,IAAI,aAAa,KAAA,GAAW,WAAW,EAAC;wBACxC,SAAS,IAAA,IAAQ;oBACnB;oBAEA,MAAM,kBACJ,4BAA4B,MAAM,aAAA,IAAiB,OAC/C;wBACE;4BACE,MAAM;4BACN,IAAI,CAAA,GAAA,kKAAA,CAAA,aAAA;4BACJ,UAAU,MAAM,aAAA;4BAChB,OAAO;wBACT;qBACF,GACA,MAAM,UAAA;oBAEZ,IAAI,mBAAmB,MAAM;wBAC3B,KAAA,MAAW,iBAAiB,gBAAiB;4BAC3C,MAAM,QAAQ,cAAc,KAAA;4BAG5B,IAAI,SAAA,CAAU,MAAK,IAAK,MAAM;gCAC5B,IAAI,cAAc,IAAA,KAAS,YAAY;oCACrC,MAAM,IAAI,yJAAA,CAAA,2BAAA,CAAyB;wCACjC,MAAM;wCACN,SAAS,CAAA,yBAAA,CAAA;oCACX;gCACF;gCAEA,IAAI,cAAc,EAAA,IAAM,MAAM;oCAC5B,MAAM,IAAI,yJAAA,CAAA,2BAAA,CAAyB;wCACjC,MAAM;wCACN,SAAS,CAAA,6BAAA,CAAA;oCACX;gCACF;gCAEA,IAAA,CAAA,CAAI,KAAA,cAAc,QAAA,KAAd,OAAA,KAAA,IAAA,GAAwB,IAAA,KAAQ,MAAM;oCACxC,MAAM,IAAI,yJAAA,CAAA,2BAAA,CAAyB;wCACjC,MAAM;wCACN,SAAS,CAAA,wCAAA,CAAA;oCACX;gCACF;gCAEA,SAAA,CAAU,MAAK,GAAI;oCACjB,IAAI,cAAc,EAAA;oCAClB,MAAM;oCACN,UAAU;wCACR,MAAM,cAAc,QAAA,CAAS,IAAA;wCAC7B,WAAA,CAAW,KAAA,cAAc,QAAA,CAAS,SAAA,KAAvB,OAAA,KAAoC;oCACjD;oCACA,aAAa;gCACf;gCAEA,MAAMC,YAAW,SAAA,CAAU,MAAK;gCAEhC,IAAA,CAAA,CACE,KAAAA,UAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAA,KAAQ,QAAA,CAAA,CAC3B,KAAAA,UAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,SAAA,KAAa,MAChC;oCAEA,IAAIA,UAAS,QAAA,CAAS,SAAA,CAAU,MAAA,GAAS,GAAG;wCAC1C,WAAW,OAAA,CAAQ;4CACjB,MAAM;4CACN,cAAc;4CACd,YAAYA,UAAS,EAAA;4CACrB,UAAUA,UAAS,QAAA,CAAS,IAAA;4CAC5B,eAAeA,UAAS,QAAA,CAAS,SAAA;wCACnC;oCACF;oCAIA,IAAI,CAAA,GAAA,kKAAA,CAAA,iBAAA,EAAeA,UAAS,QAAA,CAAS,SAAS,GAAG;wCAC/C,WAAW,OAAA,CAAQ;4CACjB,MAAM;4CACN,cAAc;4CACd,YAAA,CAAY,KAAAA,UAAS,EAAA,KAAT,OAAA,KAAe,CAAA,GAAA,kKAAA,CAAA,aAAA;4CAC3B,UAAUA,UAAS,QAAA,CAAS,IAAA;4CAC5B,MAAMA,UAAS,QAAA,CAAS,SAAA;wCAC1B;wCACAA,UAAS,WAAA,GAAc;oCACzB;gCACF;gCAEA;4BACF;4BAGA,MAAM,WAAW,SAAA,CAAU,MAAK;4BAEhC,IAAI,SAAS,WAAA,EAAa;gCACxB;4BACF;4BAEA,IAAA,CAAA,CAAI,KAAA,cAAc,QAAA,KAAd,OAAA,KAAA,IAAA,GAAwB,SAAA,KAAa,MAAM;gCAC7C,SAAS,QAAA,CAAU,SAAA,IAAA,CACjB,KAAA,CAAA,KAAA,cAAc,QAAA,KAAd,OAAA,KAAA,IAAA,GAAwB,SAAA,KAAxB,OAAA,KAAqC;4BACzC;4BAGA,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,cAAc;gCACd,YAAY,SAAS,EAAA;gCACrB,UAAU,SAAS,QAAA,CAAS,IAAA;gCAC5B,eAAA,CAAe,KAAA,cAAc,QAAA,CAAS,SAAA,KAAvB,OAAA,KAAoC;4BACrD;4BAGA,IAAA,CAAA,CACE,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAA,KAAQ,QAAA,CAAA,CAC3B,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,SAAA,KAAa,QAChC,CAAA,GAAA,kKAAA,CAAA,iBAAA,EAAe,SAAS,QAAA,CAAS,SAAS,GAC1C;gCACA,WAAW,OAAA,CAAQ;oCACjB,MAAM;oCACN,cAAc;oCACd,YAAA,CAAY,KAAA,SAAS,EAAA,KAAT,OAAA,KAAe,CAAA,GAAA,kKAAA,CAAA,aAAA;oCAC3B,UAAU,SAAS,QAAA,CAAS,IAAA;oCAC5B,MAAM,SAAS,QAAA,CAAS,SAAA;gCAC1B;gCACA,SAAS,WAAA,GAAc;4BACzB;wBACF;oBACF;gBACF;gBAEA,OAAM,UAAA;oBAvvBhB,IAAA,IAAA;oBAwvBY,WAAW,OAAA,CAAQ;wBACjB,MAAM;wBACN;wBACA;wBACA,OAAO;4BACL,cAAA,CAAc,KAAA,MAAM,YAAA,KAAN,OAAA,KAAsB;4BACpC,kBAAA,CAAkB,KAAA,MAAM,gBAAA,KAAN,OAAA,KAA0B;wBAC9C;wBACA,GAAI,oBAAoB,OAAO;4BAAE;wBAAiB,IAAI,CAAC,CAAA;oBACzD;gBACF;YACF;YAEF,SAAS;gBAAE;gBAAW;YAAY;YAClC,aAAa;gBAAE,SAAS;YAAgB;YACxC,SAAS;gBAAE,MAAM,KAAK,SAAA,CAAU;YAAM;YACtC;QACF;IACF;AACF;AAEA,IAAM,yBAAyBC,oIAAAA,CAAAA,IAAAA,CAC5B,MAAA,CAAO;IACN,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC1B,mBAAmBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC9B,uBAAuBA,oIAAAA,CAAAA,IAAAA,CACpB,MAAA,CAAO;QACN,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC5B,GACC,OAAA;IACH,2BAA2BA,oIAAAA,CAAAA,IAAAA,CACxB,MAAA,CAAO;QACN,kBAAkBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QAC7B,4BAA4BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QACvC,4BAA4BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACzC,GACC,OAAA;AACL,GACC,OAAA;AAIH,IAAM,2BAA2BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACxC,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACf,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACpB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAClB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACTA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACP,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YAChB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ,aAAa,OAAA;YAC7B,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;YACpB,eAAeA,oIAAAA,CAAAA,IAAAA,CACZ,MAAA,CAAO;gBACN,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACb,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACV,GACC,OAAA;YACH,YAAYA,oIAAAA,CAAAA,IAAAA,CACT,KAAA,CACCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;gBACP,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;gBACf,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;gBAChB,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;oBACjB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACR,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACf;YACF,IAED,OAAA;QACL;QACA,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACT,UAAUA,oIAAAA,CAAAA,IAAAA,CACP,MAAA,CAAO;YACN,SAASA,oIAAAA,CAAAA,IAAAA,CACN,KAAA,CACCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;gBACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACT,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACX,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACdA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;oBACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACT,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACb;YAEJ,IAED,QAAA;QACL,GACC,OAAA;QACH,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC5B;IAEF,OAAO;AACT;AAIA,IAAM,wBAAwBA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;IACpCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACP,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QACf,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QACpB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QAClB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACTA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,OAAOA,oIAAAA,CAAAA,IAAAA,CACJ,MAAA,CAAO;gBACN,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,IAAA,CAAK;oBAAC;iBAAY,EAAE,OAAA;gBAC5B,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;gBACpB,eAAeA,oIAAAA,CAAAA,IAAAA,CACZ,MAAA,CAAO;oBACN,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,QAAA;oBACjB,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,QAAA;gBACxB,GACC,OAAA;gBACH,YAAYA,oIAAAA,CAAAA,IAAAA,CACT,KAAA,CACCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;oBACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACT,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;oBACf,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ,YAAY,QAAA;oBAC5B,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACjB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;wBACjB,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;oBACxB;gBACF,IAED,OAAA;YACL,GACC,OAAA;YACH,UAAUA,oIAAAA,CAAAA,IAAAA,CACP,MAAA,CAAO;gBACN,SAASA,oIAAAA,CAAAA,IAAAA,CACN,KAAA,CACCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;oBACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACT,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACX,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACdA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;wBACT,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACb;gBAEJ,IAED,QAAA;YACL,GACC,OAAA;YACH,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,QAAA,GAAW,QAAA;YACrC,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACX;QAEF,OAAO;IACT;IACA;CACD;AAED,SAAS,iBAAiB,OAAA;IACxB,OACE,YAAY,QACZ,QAAQ,UAAA,CAAW,UACnB,YAAY,QACZ,QAAQ,UAAA,CAAW;AAEvB;AAEA,SAAS,aAAa,OAAA;IACpB,OAAO,QAAQ,UAAA,CAAW;AAC5B;AAEA,SAAS,qBAAqB,OAAA;IAj6B9B,IAAA,IAAA;IAk6BE,IAAI,CAAC,iBAAiB,UAAU;QAC9B,OAAO;IACT;IAEA,OAAA,CACE,KAAA,CAAA,KAAA,eAAA,CAAgB,QAAuC,KAAvD,OAAA,KAAA,IAAA,GACI,iBAAA,KADJ,OAAA,KACyB;AAE7B;AAEA,IAAM,kBAAkB;IACtB,WAAW;QACT,mBAAmB;IACrB;IACA,sBAAsB;QACpB,mBAAmB;IACrB;IACA,cAAc;QACZ,mBAAmB;IACrB;IACA,yBAAyB;QACvB,mBAAmB;IACrB;IACA,WAAW;QACT,mBAAmB;IACrB;IACA,sBAAsB;QACpB,mBAAmB;IACrB;AACF;;;;;AQz7BO,SAAS,gCAAgC,EAC9C,MAAA,EACA,WAAA,EACA,OAAO,MAAA,EACP,YAAY,WAAA,EACd;IAUE,IACE,gBAAgB,YAChB,OAAO,MAAA,KAAW,KAClB,MAAA,CAAO,EAAC,CAAE,IAAA,KAAS,UACnB,MAAA,CAAO,EAAC,CAAE,OAAA,CAAQ,MAAA,KAAW,KAC7B,MAAA,CAAO,EAAC,CAAE,OAAA,CAAQ,EAAC,CAAE,IAAA,KAAS,QAC9B;QACA,OAAO;YAAE,QAAQ,MAAA,CAAO,EAAC,CAAE,OAAA,CAAQ,EAAC,CAAE,IAAA;QAAK;IAC7C;IAGA,IAAI,OAAO;IAGX,IAAI,MAAA,CAAO,EAAC,CAAE,IAAA,KAAS,UAAU;QAC/B,QAAQ,CAAA,EAAG,MAAA,CAAO,EAAC,CAAE,OAAO,CAAA;;AAAA,CAAA;QAC5B,SAAS,OAAO,KAAA,CAAM;IACxB;IAEA,KAAA,MAAW,EAAE,IAAA,EAAM,OAAA,EAAQ,IAAK,OAAQ;QACtC,OAAQ;YACN,KAAK;gBAAU;oBACb,MAAM,IAAI,yJAAA,CAAA,qBAAA,CAAmB;wBAC3B,SAAS;wBACT;oBACF;gBACF;YAEA,KAAK;gBAAQ;oBACX,MAAM,cAAc,QACjB,GAAA,CAAI,CAAA;wBACH,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,OAAO,KAAK,IAAA;gCACd;4BACA,KAAK;gCAAS;oCACZ,MAAM,IAAIJ,yJAAAA,CAAAA,gCAAAA,CAA8B;wCACtC,eAAe;oCACjB;gCACF;wBACF;oBACF,GACC,IAAA,CAAK;oBAER,QAAQ,CAAA,EAAG,KAAI;AAAA,EAAM,YAAW;;AAAA,CAAA;oBAChC;gBACF;YAEA,KAAK;gBAAa;oBAChB,MAAM,mBAAmB,QACtB,GAAA,CAAI,CAAA;wBACH,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,OAAO,KAAK,IAAA;gCACd;4BACA,KAAK;gCAAa;oCAChB,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;wCACtC,eAAe;oCACjB;gCACF;wBACF;oBACF,GACC,IAAA,CAAK;oBAER,QAAQ,CAAA,EAAG,UAAS;AAAA,EAAM,iBAAgB;;AAAA,CAAA;oBAC1C;gBACF;YAEA,KAAK;gBAAQ;oBACX,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;wBACtC,eAAe;oBACjB;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAGA,QAAQ,CAAA,EAAG,UAAS;AAAA,CAAA;IAEpB,OAAO;QACL,QAAQ;QACR,eAAe;YAAC,CAAA;AAAA,EAAK,KAAI,CAAA,CAAG;SAAA;IAC9B;AACF;;ACrGO,SAAS,4BACd,QAAA;IAEA,OAAO,YAAA,OAAA,KAAA,IAAA,SAAU,MAAA,CAAO,GAAA,CAAI,CAAC,OAAO,QAAA,CAAW;YAC7C;YACA,SAAS,SAAS,cAAA,CAAe,MAAK;YACtC,aAAa,SAAS,YAAA,GAClB,OAAO,OAAA,CAAQ,SAAS,YAAA,CAAa,MAAM,EAAE,GAAA,CAC3C,CAAC,CAACH,QAAO,QAAO,GAAA,CAAO;oBACrB,OAAAA;oBACA;gBACF,CAAA,KAEF,EAAC;QACP,CAAA;AACF;;AFeO,IAAM,gCAAN;IASL,YACE,OAAA,EACA,QAAA,EACA,MAAA,CACA;QAZF,IAAA,CAAS,oBAAA,GAAuB;QAChC,IAAA,CAAS,2BAAA,GAA8B,KAAA;QAYrC,IAAA,CAAK,OAAA,GAAU;QACf,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,MAAA,GAAS;IAChB;IAEA,IAAI,WAAmB;QACrB,OAAO,IAAA,CAAK,MAAA,CAAO,QAAA;IACrB;IAEQ,QAAQ,EACd,IAAA,EACA,WAAA,EACA,MAAA,EACA,SAAA,EACA,WAAA,EACA,IAAA,EACA,IAAA,EACA,gBAAA,EACA,eAAA,EACA,eAAe,iBAAA,EACf,cAAA,EACA,IAAA,EACF,EAAiD;QA1EnD,IAAA;QA2EI,MAAM,OAAO,KAAK,IAAA;QAElB,MAAM,WAAyC,EAAC;QAEhD,IAAI,QAAQ,MAAM;YAChB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAI,kBAAkB,QAAQ,eAAe,IAAA,KAAS,QAAQ;YAC5D,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;gBACT,SAAS;YACX;QACF;QAEA,MAAM,EAAE,QAAQ,gBAAA,EAAkB,aAAA,EAAc,GAC9C,gCAAgC;YAAE;YAAQ;QAAY;QAExD,MAAM,OAAO;eAAK,iBAAA,OAAA,gBAAiB,EAAC;eAAQ,qBAAA,OAAA,oBAAqB,EAAG;SAAA;QAEpE,MAAM,WAAW;YAAA,YAAA;YAEf,OAAO,IAAA,CAAK,OAAA;YAAA,2BAAA;YAGZ,MAAM,IAAA,CAAK,QAAA,CAAS,IAAA;YACpB,YAAY,IAAA,CAAK,QAAA,CAAS,SAAA;YAC1B,UACE,OAAO,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,WAC9B,IAAA,CAAK,QAAA,CAAS,QAAA,GACd,OAAO,IAAA,CAAK,QAAA,CAAS,QAAA,KAAa,YAChC,IAAA,CAAK,QAAA,CAAS,QAAA,GACZ,IACA,KAAA,IACF,KAAA;YACR,QAAQ,IAAA,CAAK,QAAA,CAAS,MAAA;YACtB,MAAM,IAAA,CAAK,QAAA,CAAS,IAAA;YAAA,yBAAA;YAGpB,YAAY;YACZ;YACA,OAAO;YACP,mBAAmB;YACnB,kBAAkB;YAClB;YAAA,UAAA;YAGA,QAAQ;YAAA,kBAAA;YAGR,MAAM,KAAK,MAAA,GAAS,IAAI,OAAO,KAAA;QACjC;QAEA,OAAQ;YACN,KAAK;gBAAW;oBACd,IAAA,CAAI,KAAA,KAAK,KAAA,KAAL,OAAA,KAAA,IAAA,GAAY,MAAA,EAAQ;wBACtB,MAAM,IAAIG,yJAAAA,CAAAA,gCAAAA,CAA8B;4BACtC,eAAe;wBACjB;oBACF;oBAEA,IAAI,KAAK,UAAA,EAAY;wBACnB,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;4BACtC,eAAe;wBACjB;oBACF;oBAEA,OAAO;wBAAE,MAAM;wBAAU;oBAAS;gBACpC;YAEA,KAAK;gBAAe;oBAClB,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;wBACtC,eAAe;oBACjB;gBACF;YAEA,KAAK;gBAAe;oBAClB,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;wBACtC,eAAe;oBACjB;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAEA,MAAM,WACJ,OAAA,EAC6D;QAC7D,MAAM,EAAE,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAExC,MAAM,EACJ,eAAA,EACA,OAAO,QAAA,EACP,UAAU,WAAA,EACZ,GAAI,MAAMK,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YACtB,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D,MAAM;YACN,uBAAuB;YACvB,2BAA2BC,CAAAA,GAAAA,kKAAAA,CAAAA,4BAAAA,EACzB;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,EAAE,QAAQ,SAAA,EAAW,GAAG,aAAY,GAAI;QAC9C,MAAM,SAAS,SAAS,OAAA,CAAQ,EAAC;QAEjC,OAAO;YACL,MAAM,OAAO,IAAA;YACb,OAAO;gBACL,cAAc,SAAS,KAAA,CAAM,aAAA;gBAC7B,kBAAkB,SAAS,KAAA,CAAM,iBAAA;YACnC;YACA,cAAc,sBAAsB,OAAO,aAAa;YACxD,UAAU,4BAA4B,OAAO,QAAQ;YACrD,SAAS;gBAAE;gBAAW;YAAY;YAClC,aAAa;gBAAE,SAAS;gBAAiB,MAAM;YAAY;YAC3D,UAAU,oBAAoB;YAC9B;YACA,SAAS;gBAAE,MAAM,KAAK,SAAA,CAAU;YAAM;QACxC;IACF;IAEA,MAAM,SACJ,OAAA,EAC2D;QAC3D,MAAM,EAAE,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAExC,MAAM,OAAO;YACX,GAAG,IAAA;YACH,QAAQ;YAAA,iEAAA;YAGR,gBACE,IAAA,CAAK,MAAA,CAAO,aAAA,KAAkB,WAC1B;gBAAE,eAAe;YAAK,IACtB,KAAA;QACR;QAEA,MAAM,EAAE,eAAA,EAAiB,OAAO,QAAA,EAAS,GAAI,MAAMF,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YAC/D,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D;YACA,uBAAuB;YACvB,2BAA2BE,CAAAA,GAAAA,kKAAAA,CAAAA,mCAAAA,EACzB;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,EAAE,QAAQ,SAAA,EAAW,GAAG,aAAY,GAAI;QAE9C,IAAI,eAA4C;QAChD,IAAI,QAA4D;YAC9D,cAAc,OAAO,GAAA;YACrB,kBAAkB,OAAO,GAAA;QAC3B;QACA,IAAI;QACJ,IAAI,eAAe;QAEnB,OAAO;YACL,QAAQ,SAAS,WAAA,CACf,IAAI,gBAGF;gBACA,WAAU,KAAA,EAAO,UAAA;oBAEf,IAAI,CAAC,MAAM,OAAA,EAAS;wBAClB,eAAe;wBACf,WAAW,OAAA,CAAQ;4BAAE,MAAM;4BAAS,OAAO,MAAM,KAAA;wBAAM;wBACvD;oBACF;oBAEA,MAAM,QAAQ,MAAM,KAAA;oBAGpB,IAAI,WAAW,OAAO;wBACpB,eAAe;wBACf,WAAW,OAAA,CAAQ;4BAAE,MAAM;4BAAS,OAAO,MAAM,KAAA;wBAAM;wBACvD;oBACF;oBAEA,IAAI,cAAc;wBAChB,eAAe;wBAEf,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,GAAG,oBAAoB,MAAK;wBAC9B;oBACF;oBAEA,IAAI,MAAM,KAAA,IAAS,MAAM;wBACvB,QAAQ;4BACN,cAAc,MAAM,KAAA,CAAM,aAAA;4BAC1B,kBAAkB,MAAM,KAAA,CAAM,iBAAA;wBAChC;oBACF;oBAEA,MAAM,SAAS,MAAM,OAAA,CAAQ,EAAC;oBAE9B,IAAA,CAAI,UAAA,OAAA,KAAA,IAAA,OAAQ,aAAA,KAAiB,MAAM;wBACjC,eAAe,sBAAsB,OAAO,aAAa;oBAC3D;oBAEA,IAAA,CAAI,UAAA,OAAA,KAAA,IAAA,OAAQ,IAAA,KAAQ,MAAM;wBACxB,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,WAAW,OAAO,IAAA;wBACpB;oBACF;oBAEA,MAAM,iBAAiB,4BACrB,UAAA,OAAA,KAAA,IAAA,OAAQ,QAAA;oBAEV,IAAI,kBAAA,OAAA,KAAA,IAAA,eAAgB,MAAA,EAAQ;wBAC1B,IAAI,aAAa,KAAA,GAAW,WAAW,EAAC;wBACxC,SAAS,IAAA,IAAQ;oBACnB;gBACF;gBAEA,OAAM,UAAA;oBACJ,WAAW,OAAA,CAAQ;wBACjB,MAAM;wBACN;wBACA;wBACA;oBACF;gBACF;YACF;YAEF,SAAS;gBAAE;gBAAW;YAAY;YAClC,aAAa;gBAAE,SAAS;YAAgB;YACxC;YACA,SAAS;gBAAE,MAAM,KAAK,SAAA,CAAU;YAAM;QACxC;IACF;AACF;AAIA,IAAM,iCAAiCJ,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC9C,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACf,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACpB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAClB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACTA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACR,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACjB,UAAUA,oIAAAA,CAAAA,IAAAA,CACP,MAAA,CAAO;YACN,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YAClB,gBAAgBA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YAC1B,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,IAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,KAAW,QAAA;QAC1D,GACC,OAAA;IACL;IAEF,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACd,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACjB,mBAAmBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACvB;AACF;AAIA,IAAM,8BAA8BA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;IAC1CA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACP,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QACf,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QACpB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;QAClB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACTA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACR,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;YAC1B,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACT,UAAUA,oIAAAA,CAAAA,IAAAA,CACP,MAAA,CAAO;gBACN,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBAClB,gBAAgBA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBAC1B,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,IAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,KAAW,QAAA;YAC1D,GACC,OAAA;QACL;QAEF,OAAOA,oIAAAA,CAAAA,IAAAA,CACJ,MAAA,CAAO;YACN,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACjB,mBAAmBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACvB,GACC,OAAA;IACL;IACA;CACD;;;;AGhXM,IAAM,uBAAN;IAmBL,YACE,OAAA,EACA,QAAA,EACA,MAAA,CACA;QAtBF,IAAA,CAAS,oBAAA,GAAuB;QAuB9B,IAAA,CAAK,OAAA,GAAU;QACf,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,MAAA,GAAS;IAChB;IApBA,IAAI,WAAmB;QACrB,OAAO,IAAA,CAAK,MAAA,CAAO,QAAA;IACrB;IAEA,IAAI,uBAA+B;QA5BrC,IAAA;QA6BI,OAAA,CAAO,KAAA,IAAA,CAAK,QAAA,CAAS,oBAAA,KAAd,OAAA,KAAsC;IAC/C;IAEA,IAAI,wBAAiC;QAhCvC,IAAA;QAiCI,OAAA,CAAO,KAAA,IAAA,CAAK,QAAA,CAAS,qBAAA,KAAd,OAAA,KAAuC;IAChD;IAYA,MAAM,QAAQ,EACZ,MAAA,EACA,OAAA,EACA,WAAA,EACF,EAEE;QACA,IAAI,OAAO,MAAA,GAAS,IAAA,CAAK,oBAAA,EAAsB;YAC7C,MAAM,IAAI,yJAAA,CAAA,qCAAA,CAAmC;gBAC3C,UAAU,IAAA,CAAK,QAAA;gBACf,SAAS,IAAA,CAAK,OAAA;gBACd,sBAAsB,IAAA,CAAK,oBAAA;gBAC3B;YACF;QACF;QAEA,MAAM,EAAE,eAAA,EAAiB,OAAO,QAAA,EAAS,GAAI,MAAMC,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YAC/D,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW;YAC/C,MAAM;gBACJ,OAAO,IAAA,CAAK,OAAA;gBACZ,OAAO;gBACP,iBAAiB;gBACjB,YAAY,IAAA,CAAK,QAAA,CAAS,UAAA;gBAC1B,MAAM,IAAA,CAAK,QAAA,CAAS,IAAA;YACtB;YACA,uBAAuB;YACvB,2BAA2BC,CAAAA,GAAAA,kKAAAA,CAAAA,4BAAAA,EACzB;YAEF;YACA,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,OAAO;YACL,YAAY,SAAS,IAAA,CAAK,GAAA,CAAI,CAAA,OAAQ,KAAK,SAAS;YACpD,OAAO,SAAS,KAAA,GACZ;gBAAE,QAAQ,SAAS,KAAA,CAAM,aAAA;YAAc,IACvC,KAAA;YACJ,aAAa;gBAAE,SAAS;YAAgB;QAC1C;IACF;AACF;AAIA,IAAM,oCAAoCH,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACjD,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QAAE,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAAU;IACxD,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QAAE,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAAS,GAAG,OAAA;AACjD;;;;AE/FO,IAAM,wBAA4D;IACvE,YAAY;IACZ,YAAY;AACd;;ADeO,IAAM,mBAAN;IAaL,YACW,OAAA,EACQ,QAAA,EACA,MAAA,CACjB;QAHS,IAAA,CAAA,OAAA,GAAA;QACQ,IAAA,CAAA,QAAA,GAAA;QACA,IAAA,CAAA,MAAA,GAAA;QAfnB,IAAA,CAAS,oBAAA,GAAuB;IAgB7B;IAdH,IAAI,mBAA2B;QAxBjC,IAAA,IAAA;QAyBI,OAAA,CACE,KAAA,CAAA,KAAA,IAAA,CAAK,QAAA,CAAS,gBAAA,KAAd,OAAA,KAAkC,qBAAA,CAAsB,IAAA,CAAK,OAAO,CAAA,KAApE,OAAA,KAAyE;IAE7E;IAEA,IAAI,WAAmB;QACrB,OAAO,IAAA,CAAK,MAAA,CAAO,QAAA;IACrB;IAQA,MAAM,WAAW,EACf,MAAA,EACA,CAAA,EACA,IAAA,EACA,WAAA,EACA,IAAA,EACA,eAAA,EACA,OAAA,EACA,WAAA,EACF,EAEE;QAnDJ,IAAA,IAAA,IAAA,IAAA;QAoDI,MAAM,WAA2C,EAAC;QAElD,IAAI,eAAe,MAAM;YACvB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;gBACT,SACE;YACJ;QACF;QAEA,IAAI,QAAQ,MAAM;YAChB,SAAS,IAAA,CAAK;gBAAE,MAAM;gBAAuB,SAAS;YAAO;QAC/D;QAEA,MAAM,cAAA,CAAc,KAAA,CAAA,KAAA,CAAA,KAAA,IAAA,CAAK,MAAA,CAAO,SAAA,KAAZ,OAAA,KAAA,IAAA,GAAuB,WAAA,KAAvB,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,GAAA,KAAA,OAAA,KAA0C,aAAA,GAAA,IAAI;QAClE,MAAM,EAAE,OAAO,QAAA,EAAU,eAAA,EAAgB,GAAI,MAAMC,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YAC/D,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW;YAC/C,MAAM;gBACJ,OAAO,IAAA,CAAK,OAAA;gBACZ;gBACA;gBACA;gBACA,GAAA,CAAI,KAAA,gBAAgB,MAAA,KAAhB,OAAA,KAA0B,CAAC,CAAA;gBAC/B,iBAAiB;YACnB;YACA,uBAAuB;YACvB,2BAA2BC,CAAAA,GAAAA,kKAAAA,CAAAA,4BAAAA,EACzB;YAEF;YACA,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,OAAO;YACL,QAAQ,SAAS,IAAA,CAAK,GAAA,CAAI,CAAA,OAAQ,KAAK,QAAQ;YAC/C;YACA,UAAU;gBACR,WAAW;gBACX,SAAS,IAAA,CAAK,OAAA;gBACd,SAAS;YACX;QACF;IACF;AACF;AAIA,IAAM,4BAA4BH,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACzC,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QAAE,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAAS;AAChD;;;;;AGlGO,SAAS,iCAAiC,EAC/C,MAAA,EACA,iBAAA,EACF;IAOE,MAAM,WAAkC,EAAC;IACzC,MAAM,WAA8C,EAAC;IAErD,KAAA,MAAW,EAAE,IAAA,EAAM,OAAA,EAAQ,IAAK,OAAQ;QACtC,OAAQ;YACN,KAAK;gBAAU;oBACb,OAAQ;wBACN,KAAK;4BAAU;gCACb,SAAS,IAAA,CAAK;oCAAE,MAAM;oCAAU;gCAAQ;gCACxC;4BACF;wBACA,KAAK;4BAAa;gCAChB,SAAS,IAAA,CAAK;oCAAE,MAAM;oCAAa;gCAAQ;gCAC3C;4BACF;wBACA,KAAK;4BAAU;gCACb,SAAS,IAAA,CAAK;oCACZ,MAAM;oCACN,SAAS;gCACX;gCACA;4BACF;wBACA;4BAAS;gCACP,MAAM,mBAA0B;gCAChC,MAAM,IAAI,MACR,CAAA,iCAAA,EAAoC,iBAAgB,CAAA;4BAExD;oBACF;oBACA;gBACF;YAEA,KAAK;gBAAQ;oBACX,SAAS,IAAA,CAAK;wBACZ,MAAM;wBACN,SAAS,QAAQ,GAAA,CAAI,CAAC,MAAM;4BArDtC,IAAA,IAAA,IAAA,IAAA;4BAsDY,OAAQ,KAAK,IAAA;gCACX,KAAK;oCAAQ;wCACX,OAAO;4CAAE,MAAM;4CAAc,MAAM,KAAK,IAAA;wCAAK;oCAC/C;gCACA,KAAK;oCAAS;wCACZ,OAAO;4CACL,MAAM;4CACN,WACE,KAAK,KAAA,YAAiB,MAClB,KAAK,KAAA,CAAM,QAAA,KACX,CAAA,KAAA,EAAA,CACE,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,aACnB,QAAA,EAAWK,CAAAA,GAAAA,kKAAAA,CAAAA,4BAAAA,EAA0B,KAAK,KAAK,EAAC,CAAA;4CAAA,0CAAA;4CAGtD,QAAA,CAAQ,KAAA,CAAA,KAAA,KAAK,gBAAA,KAAL,OAAA,KAAA,IAAA,GAAuB,MAAA,KAAvB,OAAA,KAAA,IAAA,GAA+B,WAAA;wCACzC;oCACF;gCACA,KAAK;oCAAQ;wCACX,IAAI,KAAK,IAAA,YAAgB,KAAK;4CAE5B,MAAM,IAAIT,yJAAAA,CAAAA,gCAAAA,CAA8B;gDACtC,eAAe;4CACjB;wCACF;wCAEA,OAAQ,KAAK,QAAA;4CACX,KAAK;gDAAmB;oDACtB,OAAO;wDACL,MAAM;wDACN,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,CAAA,KAAA,EAAQ,MAAK,IAAA,CAAA;wDACxC,WAAW,CAAA,4BAAA,EAA+B,KAAK,IAAI,CAAA,CAAA;oDACrD;gDACF;4CACA;gDAAS;oDACP,MAAM,IAAIA,yJAAAA,CAAAA,gCAAAA,CAA8B;wDACtC,eACE;oDACJ;gDACF;wCACF;oCACF;4BACF;wBACF;oBACF;oBAEA;gBACF;YAEA,KAAK;gBAAa;oBAChB,KAAA,MAAW,QAAQ,QAAS;wBAC1B,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,SAAS,IAAA,CAAK;wCACZ,MAAM;wCACN,SAAS;4CAAC;gDAAE,MAAM;gDAAe,MAAM,KAAK,IAAA;4CAAK;yCAAC;oCACpD;oCACA;gCACF;4BACA,KAAK;gCAAa;oCAChB,SAAS,IAAA,CAAK;wCACZ,MAAM;wCACN,SAAS,KAAK,UAAA;wCACd,MAAM,KAAK,QAAA;wCACX,WAAW,KAAK,SAAA,CAAU,KAAK,IAAI;oCACrC;oCACA;gCACF;wBACF;oBACF;oBAEA;gBACF;YAEA,KAAK;gBAAQ;oBACX,KAAA,MAAW,QAAQ,QAAS;wBAC1B,SAAS,IAAA,CAAK;4BACZ,MAAM;4BACN,SAAS,KAAK,UAAA;4BACd,QAAQ,KAAK,SAAA,CAAU,KAAK,MAAM;wBACpC;oBACF;oBAEA;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAEA,OAAO;QAAE;QAAU;IAAS;AAC9B;;AClJO,SAAS,8BAA8B,EAC5C,YAAA,EACA,YAAA,EACF;IAIE,OAAQ;QACN,KAAK,KAAA;QACL,KAAK;YACH,OAAO,eAAe,eAAe;QACvC,KAAK;YACH,OAAO;QACT,KAAK;YACH,OAAO;QACT;YACE,OAAO,eAAe,eAAe;IACzC;AACF;;ACbO,SAAS,sBAAsB,EACpC,IAAA,EACA,MAAA,EACF;IAVA,IAAA;IAyBE,MAAM,QAAA,CAAA,CAAQ,KAAA,KAAK,KAAA,KAAL,OAAA,KAAA,IAAA,GAAY,MAAA,IAAS,KAAK,KAAA,GAAQ,KAAA;IAEhD,MAAM,eAA6C,EAAC;IAEpD,IAAI,SAAS,MAAM;QACjB,OAAO;YAAE,OAAO,KAAA;YAAW,aAAa,KAAA;YAAW;QAAa;IAClE;IAEA,MAAM,aAAa,KAAK,UAAA;IAExB,MAAMC,eAA0C,EAAC;IAEjD,KAAA,MAAW,QAAQ,MAAO;QACxB,OAAQ,KAAK,IAAA;YACX,KAAK;gBACHA,aAAY,IAAA,CAAK;oBACf,MAAM;oBACN,MAAM,KAAK,IAAA;oBACX,aAAa,KAAK,WAAA;oBAClB,YAAY,KAAK,UAAA;oBACjB,QAAQ,SAAS,OAAO,KAAA;gBAC1B;gBACA;YACF,KAAK;gBACH,OAAQ,KAAK,EAAA;oBACX,KAAK;wBACHA,aAAY,IAAA,CAAK;4BACf,MAAM;4BACN,qBAAqB,KAAK,IAAA,CAAK,iBAAA;4BAI/B,eAAe,KAAK,IAAA,CAAK,YAAA;wBAK3B;wBACA;oBACF;wBACE,aAAa,IAAA,CAAK;4BAAE,MAAM;4BAAoB;wBAAK;wBACnD;gBACJ;gBACA;YACF;gBACE,aAAa,IAAA,CAAK;oBAAE,MAAM;oBAAoB;gBAAK;gBACnD;QACJ;IACF;IAEA,IAAI,cAAc,MAAM;QACtB,OAAO;YAAE,OAAOA;YAAa,aAAa,KAAA;YAAW;QAAa;IACpE;IAEA,MAAM,OAAO,WAAW,IAAA;IAExB,OAAQ;QACN,KAAK;QACL,KAAK;QACL,KAAK;YACH,OAAO;gBAAE,OAAOA;gBAAa,aAAa;gBAAM;YAAa;QAC/D,KAAK;YACH,OAAO;gBACL,OAAOA;gBACP,aAAa;oBACX,MAAM;oBACN,MAAM,WAAW,QAAA;gBACnB;gBACA;YACF;QACF;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAID,yJAAAA,CAAAA,gCAAAA,CAA8B;oBACtC,eAAe,CAAA,8BAAA,EAAiC,iBAAgB,CAAA;gBAClE;YACF;IACF;AACF;;AH/EO,IAAM,+BAAN;IAQL,YAAY,OAAA,EAAiC,MAAA,CAAsB;QAPnE,IAAA,CAAS,oBAAA,GAAuB;QAChC,IAAA,CAAS,2BAAA,GAA8B;QAOrC,IAAA,CAAK,OAAA,GAAU;QACf,IAAA,CAAK,MAAA,GAAS;IAChB;IAEA,IAAI,WAAmB;QACrB,OAAO,IAAA,CAAK,MAAA,CAAO,QAAA;IACrB;IAEQ,QAAQ,EACd,IAAA,EACA,SAAA,EACA,WAAA,EACA,aAAA,EACA,IAAA,EACA,IAAA,EACA,eAAA,EACA,gBAAA,EACA,IAAA,EACA,MAAA,EACA,gBAAA,EACA,cAAA,EACF,EAAiD;QArDnD,IAAA,IAAA,IAAA;QAsDI,MAAM,WAAyC,EAAC;QAChD,MAAM,cAAc,wBAAwB,IAAA,CAAK,OAAO;QACxD,MAAM,OAAO,KAAK,IAAA;QAElB,IAAI,QAAQ,MAAM;YAChB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAI,QAAQ,MAAM;YAChB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAI,mBAAmB,MAAM;YAC3B,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAI,oBAAoB,MAAM;YAC5B,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,IAAI,iBAAiB,MAAM;YACzB,SAAS,IAAA,CAAK;gBACZ,MAAM;gBACN,SAAS;YACX;QACF;QAEA,MAAM,EAAE,QAAA,EAAU,UAAU,eAAA,EAAgB,GAC1C,iCAAiC;YAC/B;YACA,mBAAmB,YAAY,iBAAA;QACjC;QAEF,SAAS,IAAA,IAAQ;QAEjB,MAAM,gBAAgB,CAAA,GAAA,kKAAA,CAAA,uBAAA,EAAqB;YACzC,UAAU;YACV,iBAAiB;YACjB,QAAQ;QACV;QAEA,MAAM,WAAA,CAAW,KAAA,iBAAA,OAAA,KAAA,IAAA,cAAe,aAAA,KAAf,OAAA,KAAgC;QAEjD,MAAM,WAAW;YACf,OAAO,IAAA,CAAK,OAAA;YACZ,OAAO;YACP;YACA,OAAO;YACP,mBAAmB;YAEnB,GAAA,CAAI,kBAAA,OAAA,KAAA,IAAA,eAAgB,IAAA,MAAS,UAAU;gBACrC,MAAM;oBACJ,QACE,eAAe,MAAA,IAAU,OACrB;wBACE,MAAM;wBACN,QAAQ;wBACR,MAAA,CAAM,KAAA,eAAe,IAAA,KAAf,OAAA,KAAuB;wBAC7B,aAAa,eAAe,WAAA;wBAC5B,QAAQ,eAAe,MAAA;oBACzB,IACA;wBAAE,MAAM;oBAAc;gBAC9B;YACF,CAAA;YAAA,oBAAA;YAGA,UAAU,iBAAA,OAAA,KAAA,IAAA,cAAe,QAAA;YACzB,qBAAqB,iBAAA,OAAA,KAAA,IAAA,cAAe,iBAAA;YACpC,sBAAsB,iBAAA,OAAA,KAAA,IAAA,cAAe,kBAAA;YACrC,OAAO,iBAAA,OAAA,KAAA,IAAA,cAAe,KAAA;YACtB,MAAM,iBAAA,OAAA,KAAA,IAAA,cAAe,IAAA;YACrB,cAAc,iBAAA,OAAA,KAAA,IAAA,cAAe,YAAA;YAAA,2BAAA;YAG7B,GAAI,YAAY,gBAAA,IAAA,CACd,iBAAA,OAAA,KAAA,IAAA,cAAe,eAAA,KAAmB,QAAQ;gBACxC,WAAW;oBAAE,QAAQ,iBAAA,OAAA,KAAA,IAAA,cAAe,eAAA;gBAAgB;YACtD,CAAA;YACF,GAAI,YAAY,sBAAA,IAA0B;gBACxC,YAAY;YACd,CAAA;QACF;QAEA,IAAI,YAAY,gBAAA,EAAkB;YAGhC,IAAI,SAAS,WAAA,IAAe,MAAM;gBAChC,SAAS,WAAA,GAAc,KAAA;gBACvB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;YAEA,IAAI,SAAS,KAAA,IAAS,MAAM;gBAC1B,SAAS,KAAA,GAAQ,KAAA;gBACjB,SAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,SAAS;oBACT,SAAS;gBACX;YACF;QACF;QAEA,OAAQ;YACN,KAAK;gBAAW;oBACd,MAAM,EAAE,KAAA,EAAO,WAAA,EAAa,YAAA,EAAa,GAAI,sBAAsB;wBACjE;wBACA,QAAQ;oBACV;oBAEA,OAAO;wBACL,MAAM;4BACJ,GAAG,QAAA;4BACH;4BACA;wBACF;wBACA,UAAU;+BAAI;+BAAa;yBAAY;oBACzC;gBACF;YAEA,KAAK;gBAAe;oBAClB,OAAO;wBACL,MAAM;4BACJ,GAAG,QAAA;4BACH,MAAM;gCACJ,QACE,KAAK,MAAA,IAAU,OACX;oCACE,MAAM;oCACN,QAAQ;oCACR,MAAA,CAAM,KAAA,KAAK,IAAA,KAAL,OAAA,KAAa;oCACnB,aAAa,KAAK,WAAA;oCAClB,QAAQ,KAAK,MAAA;gCACf,IACA;oCAAE,MAAM;gCAAc;4BAC9B;wBACF;wBACA;oBACF;gBACF;YAEA,KAAK;gBAAe;oBAClB,OAAO;wBACL,MAAM;4BACJ,GAAG,QAAA;4BACH,aAAa;gCAAE,MAAM;gCAAY,MAAM,KAAK,IAAA,CAAK,IAAA;4BAAK;4BACtD,OAAO;gCACL;oCACE,MAAM;oCACN,MAAM,KAAK,IAAA,CAAK,IAAA;oCAChB,aAAa,KAAK,IAAA,CAAK,WAAA;oCACvB,YAAY,KAAK,IAAA,CAAK,UAAA;oCACtB,QAAQ;gCACV;6BACF;wBACF;wBACA;oBACF;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;gBACzD;QACF;IACF;IAEA,MAAM,WACJ,OAAA,EAC6D;QA7OjE,IAAA,IAAA,IAAA,IAAA,IAAA;QA8OI,MAAM,EAAE,MAAM,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAE9C,MAAM,EACJ,eAAA,EACA,OAAO,QAAA,EACP,UAAU,WAAA,EACZ,GAAI,MAAMK,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YACtB,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D;YACA,uBAAuB;YACvB,2BAA2BC,CAAAA,GAAAA,kKAAAA,CAAAA,4BAAAA,EACzBH,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;gBACP,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACN,YAAYA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACd,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBACT,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACRA,oIAAAA,CAAAA,IAAAA,CAAE,kBAAA,CAAmB,QAAQ;oBAC3BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;wBAChB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;wBAChB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACTA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;4BACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;4BAChB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;4BACR,aAAaA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CACbA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;gCACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;gCAChB,aAAaA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gCACf,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gCACb,KAAKA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gCACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;4BACX;wBAEJ;oBAEJ;oBACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;wBAChB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;wBACX,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;wBACR,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;oBACf;oBACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;oBAClB;oBACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;oBAClB;oBACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;wBACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;oBAClB;iBACD;gBAEH,oBAAoBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;oBAAE,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;gBAAS,GAAG,QAAA;gBACrD,OAAO;YACT;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,qBAAqB,SAAS,MAAA,CACjC,MAAA,CAAO,CAAA,SAAU,OAAO,IAAA,KAAS,WACjC,OAAA,CAAQ,CAAA,SAAU,OAAO,OAAO,EAChC,MAAA,CAAO,CAAA,UAAW,QAAQ,IAAA,KAAS;QAEtC,MAAM,YAAY,SAAS,MAAA,CACxB,MAAA,CAAO,CAAA,SAAU,OAAO,IAAA,KAAS,iBACjC,GAAA,CAAI,CAAA,SAAA,CAAW;gBACd,cAAc;gBACd,YAAY,OAAO,OAAA;gBACnB,UAAU,OAAO,IAAA;gBACjB,MAAM,OAAO,SAAA;YACf,CAAA;QAEF,OAAO;YACL,MAAM,mBAAmB,GAAA,CAAI,CAAA,UAAW,QAAQ,IAAI,EAAE,IAAA,CAAK;YAC3D,SAAS,mBAAmB,OAAA,CAAQ,CAAA,UAClC,QAAQ,WAAA,CAAY,GAAA,CAAI,CAAA;oBAhUhC,IAAAF,KAAAQ,KAAAC;oBAgU+C,OAAA;wBACrC,YAAY;wBACZ,IAAA,CAAIA,MAAAA,CAAAD,MAAAA,CAAAR,MAAA,IAAA,CAAK,MAAA,EAAO,UAAA,KAAZ,OAAA,KAAA,IAAAQ,IAAA,IAAA,CAAAR,IAAAA,KAAA,OAAAS,MAA8BC,CAAAA,GAAAA,kKAAAA,CAAAA,aAAAA;wBAClC,KAAK,WAAW,GAAA;wBAChB,OAAO,WAAW,KAAA;oBACpB;gBAAA;YAEF,cAAc,8BAA8B;gBAC1C,cAAA,CAAc,KAAA,SAAS,kBAAA,KAAT,OAAA,KAAA,IAAA,GAA6B,MAAA;gBAC3C,cAAc,UAAU,MAAA,GAAS;YACnC;YACA,WAAW,UAAU,MAAA,GAAS,IAAI,YAAY,KAAA;YAC9C,OAAO;gBACL,cAAc,SAAS,KAAA,CAAM,YAAA;gBAC7B,kBAAkB,SAAS,KAAA,CAAM,aAAA;YACnC;YACA,SAAS;gBACP,WAAW,KAAA;gBACX,aAAa,CAAC;YAChB;YACA,aAAa;gBACX,SAAS;gBACT,MAAM;YACR;YACA,SAAS;gBACP,MAAM,KAAK,SAAA,CAAU;YACvB;YACA,UAAU;gBACR,IAAI,SAAS,EAAA;gBACb,WAAW,IAAI,KAAK,SAAS,UAAA,GAAa;gBAC1C,SAAS,SAAS,KAAA;YACpB;YACA,kBAAkB;gBAChB,QAAQ;oBACN,YAAY,SAAS,EAAA;oBACrB,oBAAA,CACE,KAAA,CAAA,KAAA,SAAS,KAAA,CAAM,oBAAA,KAAf,OAAA,KAAA,IAAA,GAAqC,aAAA,KAArC,OAAA,KAAsD;oBACxD,iBAAA,CACE,KAAA,CAAA,KAAA,SAAS,KAAA,CAAM,qBAAA,KAAf,OAAA,KAAA,IAAA,GAAsC,gBAAA,KAAtC,OAAA,KAA0D;gBAC9D;YACF;YACA;QACF;IACF;IAEA,MAAM,SACJ,OAAA,EAC2D;QAC3D,MAAM,EAAE,MAAM,IAAA,EAAM,QAAA,EAAS,GAAI,IAAA,CAAK,OAAA,CAAQ;QAE9C,MAAM,EAAE,eAAA,EAAiB,OAAO,QAAA,EAAS,GAAI,MAAMP,CAAAA,GAAAA,kKAAAA,CAAAA,gBAAAA,EAAc;YAC/D,KAAK,IAAA,CAAK,MAAA,CAAO,GAAA,CAAI;gBACnB,MAAM;gBACN,SAAS,IAAA,CAAK,OAAA;YAChB;YACA,SAASC,CAAAA,GAAAA,kKAAAA,CAAAA,iBAAAA,EAAe,IAAA,CAAK,MAAA,CAAO,OAAA,IAAW,QAAQ,OAAO;YAC9D,MAAM;gBACJ,GAAG,IAAA;gBACH,QAAQ;YACV;YACA,uBAAuB;YACvB,2BAA2BE,CAAAA,GAAAA,kKAAAA,CAAAA,mCAAAA,EACzB;YAEF,aAAa,QAAQ,WAAA;YACrB,OAAO,IAAA,CAAK,MAAA,CAAO,KAAA;QACrB;QAEA,MAAM,OAAO,IAAA;QAEb,IAAI,eAA4C;QAChD,IAAI,eAAe;QACnB,IAAI,mBAAmB;QACvB,IAAI,qBAAoC;QACxC,IAAI,kBAAiC;QACrC,IAAI,aAA4B;QAChC,MAAM,mBAGF,CAAC;QACL,IAAI,eAAe;QAEnB,OAAO;YACL,QAAQ,SAAS,WAAA,CACf,IAAI,gBAGF;gBACA,WAAU,KAAA,EAAO,UAAA;oBAxZ3B,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oBA0ZY,IAAI,CAAC,MAAM,OAAA,EAAS;wBAClB,eAAe;wBACf,WAAW,OAAA,CAAQ;4BAAE,MAAM;4BAAS,OAAO,MAAM,KAAA;wBAAM;wBACvD;oBACF;oBAEA,MAAM,QAAQ,MAAM,KAAA;oBAEpB,IAAI,+BAA+B,QAAQ;wBACzC,IAAI,MAAM,IAAA,CAAK,IAAA,KAAS,iBAAiB;4BACvC,gBAAA,CAAiB,MAAM,YAAY,CAAA,GAAI;gCACrC,UAAU,MAAM,IAAA,CAAK,IAAA;gCACrB,YAAY,MAAM,IAAA,CAAK,OAAA;4BACzB;4BAEA,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,cAAc;gCACd,YAAY,MAAM,IAAA,CAAK,OAAA;gCACvB,UAAU,MAAM,IAAA,CAAK,IAAA;gCACrB,eAAe,MAAM,IAAA,CAAK,SAAA;4BAC5B;wBACF;oBACF,OAAA,IAAW,0CAA0C,QAAQ;wBAC3D,MAAM,WAAW,gBAAA,CAAiB,MAAM,YAAY,CAAA;wBAEpD,IAAI,YAAY,MAAM;4BACpB,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,cAAc;gCACd,YAAY,SAAS,UAAA;gCACrB,UAAU,SAAS,QAAA;gCACnB,eAAe,MAAM,KAAA;4BACvB;wBACF;oBACF,OAAA,IAAW,uBAAuB,QAAQ;wBACxC,aAAa,MAAM,QAAA,CAAS,EAAA;wBAC5B,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,IAAI,MAAM,QAAA,CAAS,EAAA;4BACnB,WAAW,IAAI,KAAK,MAAM,QAAA,CAAS,UAAA,GAAa;4BAChD,SAAS,MAAM,QAAA,CAAS,KAAA;wBAC1B;oBACF,OAAA,IAAW,iBAAiB,QAAQ;wBAClC,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,WAAW,MAAM,KAAA;wBACnB;oBACF,OAAA,IACE,8BAA8B,UAC9B,MAAM,IAAA,CAAK,IAAA,KAAS,iBACpB;wBACA,gBAAA,CAAiB,MAAM,YAAY,CAAA,GAAI,KAAA;wBACvC,eAAe;wBACf,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,cAAc;4BACd,YAAY,MAAM,IAAA,CAAK,OAAA;4BACvB,UAAU,MAAM,IAAA,CAAK,IAAA;4BACrB,MAAM,MAAM,IAAA,CAAK,SAAA;wBACnB;oBACF,OAAA,IAAW,wBAAwB,QAAQ;wBACzC,eAAe,8BAA8B;4BAC3C,cAAA,CAAc,KAAA,MAAM,QAAA,CAAS,kBAAA,KAAf,OAAA,KAAA,IAAA,GAAmC,MAAA;4BACjD;wBACF;wBACA,eAAe,MAAM,QAAA,CAAS,KAAA,CAAM,YAAA;wBACpC,mBAAmB,MAAM,QAAA,CAAS,KAAA,CAAM,aAAA;wBACxC,qBAAA,CACE,KAAA,CAAA,KAAA,MAAM,QAAA,CAAS,KAAA,CAAM,oBAAA,KAArB,OAAA,KAAA,IAAA,GAA2C,aAAA,KAA3C,OAAA,KACA;wBACF,kBAAA,CACE,KAAA,CAAA,KAAA,MAAM,QAAA,CAAS,KAAA,CAAM,qBAAA,KAArB,OAAA,KAAA,IAAA,GAA4C,gBAAA,KAA5C,OAAA,KACA;oBACJ,OAAA,IAAW,+BAA+B,QAAQ;wBAChD,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,QAAQ;gCACN,YAAY;gCACZ,IAAA,CAAI,KAAA,CAAA,KAAA,CAAA,KAAA,KAAK,MAAA,EAAO,UAAA,KAAZ,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,GAAA,KAAA,OAAA,KAA8BI,CAAAA,GAAAA,kKAAAA,CAAAA,aAAAA;gCAClC,KAAK,MAAM,UAAA,CAAW,GAAA;gCACtB,OAAO,MAAM,UAAA,CAAW,KAAA;4BAC1B;wBACF;oBACF;gBACF;gBAEA,OAAM,UAAA;oBACJ,WAAW,OAAA,CAAQ;wBACjB,MAAM;wBACN;wBACA,OAAO;4BAAE;4BAAc;wBAAiB;wBACxC,GAAA,CAAK,sBAAsB,QAAQ,mBAAmB,IAAA,KAAS;4BAC7D,kBAAkB;gCAChB,QAAQ;oCACN;oCACA;oCACA;gCACF;4BACF;wBACF,CAAA;oBACF;gBACF;YACF;YAEF,SAAS;gBACP,WAAW,KAAA;gBACX,aAAa,CAAC;YAChB;YACA,aAAa;gBAAE,SAAS;YAAgB;YACxC,SAAS;gBAAE,MAAM,KAAK,SAAA,CAAU;YAAM;YACtC;QACF;IACF;AACF;AAEA,IAAM,cAAcR,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC3B,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAChB,sBAAsBA,oIAAAA,CAAAA,IAAAA,CACnB,MAAA,CAAO;QAAE,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAAU,GAC7C,OAAA;IACH,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACjB,uBAAuBA,oIAAAA,CAAAA,IAAAA,CACpB,MAAA,CAAO;QAAE,kBAAkBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAAU,GAChD,OAAA;AACL;AAEA,IAAM,uBAAuBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACpC,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;AACX;AAEA,IAAM,8BAA8BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC3C,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,IAAA,CAAK;QAAC;QAAsB;KAAsB;IAC1D,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACjB,oBAAoBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YAAE,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QAAS,GAAG,OAAA;QACrD,OAAO;IACT;AACF;AAEA,IAAM,6BAA6BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC1C,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACjB,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACN,YAAYA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACd,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACX;AACF;AAEA,IAAM,+BAA+BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC5C,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAChB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,kBAAA,CAAmB,QAAQ;QACjCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;QAClB;QACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;YAChB,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACN,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACX,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACR,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACb,QAAQA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;QACpB;KACD;AACH;AAEA,IAAM,2CAA2CA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACxD,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACX,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAChB,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;AACX;AAEA,IAAM,gCAAgCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC7C,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAChB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,kBAAA,CAAmB,QAAQ;QACjCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;QAClB;QACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;YACP,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;YAChB,IAAIA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACN,SAASA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACX,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;YACR,WAAWA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACf;KACD;AACH;AAEA,IAAM,gCAAgCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC7C,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,YAAYA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QACnB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;QAChB,KAAKA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACP,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACX;AACF;AAEA,IAAM,6BAA6BA,oIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;IACzC;IACA;IACA;IACA;IACA;IACA;IACA;IACAA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;QAAE,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA;IAAS,GAAG,WAAA;CAChC;AAED,SAAS,iBACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAEA,SAAS,8BACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAEA,SAAS,wBACP,KAAA;IAEA,OACE,MAAM,IAAA,KAAS,wBAAwB,MAAM,IAAA,KAAS;AAE1D;AAEA,SAAS,uBACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAEA,SAAS,0CACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAEA,SAAS,+BACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAEA,SAAS,+BACP,KAAA;IAEA,OAAO,MAAM,IAAA,KAAS;AACxB;AAQA,SAAS,wBAAwB,OAAA;IAE/B,IAAI,QAAQ,UAAA,CAAW,MAAM;QAC3B,IAAI,QAAQ,UAAA,CAAW,cAAc,QAAQ,UAAA,CAAW,eAAe;YACrE,OAAO;gBACL,kBAAkB;gBAClB,mBAAmB;gBACnB,wBAAwB;YAC1B;QACF;QAEA,OAAO;YACL,kBAAkB;YAClB,mBAAmB;YACnB,wBAAwB;QAC1B;IACF;IAGA,OAAO;QACL,kBAAkB;QAClB,mBAAmB;QACnB,wBAAwB;IAC1B;AACF;AAEA,IAAM,uCAAuCA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACpD,UAAUA,oIAAAA,CAAAA,IAAAA,CAAE,GAAA,GAAM,OAAA;IAClB,mBAAmBA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,GAAU,OAAA;IAC/B,oBAAoBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC/B,OAAOA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,GAAU,OAAA;IACnB,MAAMA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IACjB,iBAAiBA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;IAC5B,eAAeA,oIAAAA,CAAAA,IAAAA,CAAE,OAAA,GAAU,OAAA;IAC3B,cAAcA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,OAAA;AAC3B;;AIhsBA,IAAM,6BAA6BA,oIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO,CAAC;AAE7C,SAAS,qBAAqB,EAC5B,iBAAA,EACA,YAAA,EACF,GASI,CAAC,CAAA;IAMH,OAAO;QACL,MAAM;QACN,IAAI;QACJ,MAAM;YACJ;YACA;QACF;QACA,YAAY;IACd;AACF;AAEO,IAAM,cAAc;IACzB,kBAAkB;AACpB;;AlBqIO,SAAS,aACd,UAAkC,CAAC,CAAA;IAzKrC,IAAA,IAAA,IAAA;IA2KE,MAAM,UAAA,CACJ,KAAA,CAAA,GAAA,kKAAA,CAAA,uBAAA,EAAqB,QAAQ,OAAO,CAAA,KAApC,OAAA,KAAyC;IAG3C,MAAM,gBAAA,CAAgB,KAAA,QAAQ,aAAA,KAAR,OAAA,KAAyB;IAE/C,MAAM,eAAA,CAAe,KAAA,QAAQ,IAAA,KAAR,OAAA,KAAgB;IAErC,MAAM,aAAa,IAAA,CAAO;YACxB,eAAe,CAAA,OAAA,EAAU,CAAA,GAAA,kKAAA,CAAA,aAAA,EAAW;gBAClC,QAAQ,QAAQ,MAAA;gBAChB,yBAAyB;gBACzB,aAAa;YACf,GAAE,CAAA;YACF,uBAAuB,QAAQ,YAAA;YAC/B,kBAAkB,QAAQ,OAAA;YAC1B,GAAG,QAAQ,OAAA;QACb,CAAA;IAEA,MAAM,kBAAkB,CACtB,SACA,WAA+B,CAAC,CAAA,GAEhC,IAAI,wBAAwB,SAAS,UAAU;YAC7C,UAAU,CAAA,EAAG,aAAY,KAAA,CAAA;YACzB,KAAK,CAAC,EAAE,IAAA,EAAK,GAAM,CAAA,EAAG,QAAO,EAAG,KAAI,CAAA;YACpC,SAAS;YACT;YACA,OAAO,QAAQ,KAAA;QACjB;IAEF,MAAM,wBAAwB,CAC5B,SACA,WAAqC,CAAC,CAAA,GAEtC,IAAI,8BAA8B,SAAS,UAAU;YACnD,UAAU,CAAA,EAAG,aAAY,WAAA,CAAA;YACzB,KAAK,CAAC,EAAE,IAAA,EAAK,GAAM,CAAA,EAAG,QAAO,EAAG,KAAI,CAAA;YACpC,SAAS;YACT;YACA,OAAO,QAAQ,KAAA;QACjB;IAEF,MAAM,uBAAuB,CAC3B,SACA,WAAoC,CAAC,CAAA,GAErC,IAAI,qBAAqB,SAAS,UAAU;YAC1C,UAAU,CAAA,EAAG,aAAY,UAAA,CAAA;YACzB,KAAK,CAAC,EAAE,IAAA,EAAK,GAAM,CAAA,EAAG,QAAO,EAAG,KAAI,CAAA;YACpC,SAAS;YACT,OAAO,QAAQ,KAAA;QACjB;IAEF,MAAM,mBAAmB,CACvB,SACA,WAAgC,CAAC,CAAA,GAEjC,IAAI,iBAAiB,SAAS,UAAU;YACtC,UAAU,CAAA,EAAG,aAAY,MAAA,CAAA;YACzB,KAAK,CAAC,EAAE,IAAA,EAAK,GAAM,CAAA,EAAG,QAAO,EAAG,KAAI,CAAA;YACpC,SAAS;YACT,OAAO,QAAQ,KAAA;QACjB;IAEF,MAAM,sBAAsB,CAC1B,SACA;QAEA,IAAI,YAAY;YACd,MAAM,IAAI,MACR;QAEJ;QAEA,IAAI,YAAY,0BAA0B;YACxC,OAAO,sBACL,SACA;QAEJ;QAEA,OAAO,gBAAgB,SAAS;IAClC;IAEA,MAAM,uBAAuB,CAAC;QAC5B,OAAO,IAAI,6BAA6B,SAAS;YAC/C,UAAU,CAAA,EAAG,aAAY,UAAA,CAAA;YACzB,KAAK,CAAC,EAAE,IAAA,EAAK,GAAM,CAAA,EAAG,QAAO,EAAG,KAAI,CAAA;YACpC,SAAS;YACT,OAAO,QAAQ,KAAA;QACjB;IACF;IAEA,MAAM,WAAW,SACf,OAAA,EACA,QAAA;QAEA,OAAO,oBAAoB,SAAS;IACtC;IAEA,SAAS,aAAA,GAAgB;IACzB,SAAS,IAAA,GAAO;IAChB,SAAS,UAAA,GAAa;IACtB,SAAS,SAAA,GAAY;IACrB,SAAS,SAAA,GAAY;IACrB,SAAS,aAAA,GAAgB;IACzB,SAAS,kBAAA,GAAqB;IAE9B,SAAS,KAAA,GAAQ;IACjB,SAAS,UAAA,GAAa;IAEtB,SAAS,KAAA,GAAQ;IAEjB,OAAO;AACT;AAKO,IAAM,SAAS,aAAa;IACjC,eAAe;AACjB"}},
    {"offset": {"line": 2539, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}